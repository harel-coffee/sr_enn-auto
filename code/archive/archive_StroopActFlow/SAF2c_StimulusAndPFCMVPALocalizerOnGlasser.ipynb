{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StroopActFlow\n",
    "### Identify stimuli localizers for input layer for Stroop Model\n",
    "### Identify top-down task rule inputs (e.g., PFC) for Stroop Model\n",
    "\n",
    "\n",
    "#### Taku Ito\n",
    "#### 1/25/2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import os\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertCSVToCIFTI64k(inputfilename,outputfilename):\n",
    "    ciftitemplate = '/projects3/StroopActFlow/data/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "    wb_command = 'wb_command -cifti-convert -from-text' \n",
    "    wb_command += ' ' + inputfilename \n",
    "    wb_command += ' ' + ciftitemplate\n",
    "    wb_command += ' ' + outputfilename\n",
    "    wb_command += \" -col-delim ','\"\n",
    "    wb_command += ' -reset-scalars'\n",
    "    os.system(wb_command)\n",
    "#     print wb_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjNums = ['101', '102']\n",
    "\n",
    "basedir = '/projects3/StroopActFlow/data/'\n",
    "resultsdir = basedir + 'results/'\n",
    "restdir = resultsdir + 'glmRest_GlasserParcels/'\n",
    "\n",
    "glasser_nets = np.loadtxt('/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/community_order.txt', delimiter=',')\n",
    "# Make into python numbering (starts from 0)\n",
    "glasser_nets -= 1.0\n",
    "networkorder = glasser_nets.astype(int)\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "\n",
    "networkmappings = {'fpn':7, 'vis':1, 'smn':2, 'con':3, 'dmn':6, 'aud1':8, 'aud2':9, 'dan':11}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "networkdef = '/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/parcel_network_assignments.txt'\n",
    "networkdef = np.loadtxt(networkdef, delimiter=',')\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "    \n",
    "# Load in Glasser parcels\n",
    "glasserfile = '/projects3/StroopActFlow/data/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "glasser = nib.load(glasserfile).get_data()\n",
    "glasser = np.squeeze(glasser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Run task activation analysis on 64k Surface for four different stimuli localizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBetas(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "    # Load smoothed betas!\n",
    "    betas = np.loadtxt(datadir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k.csv',delimiter=',')\n",
    "    betas = betas[:,18:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Run 2 MVPA analysis \n",
    "### Color Localizer: Green V Red\n",
    "### Word Localizer: Green V Red\n",
    "* Stim 1: Neutral Color Green\n",
    "* Stim 2: Neutral Color Red\n",
    "* Stim 3: Neutral Word Green\n",
    "* Stim 4: Neutral Word Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load in data as dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading betas for subject 101\n",
      "Loading betas for subject 102\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 45\n",
    "nstims = 4\n",
    "colorstims = [0,1] # First 2 are color\n",
    "wordstims = [2,3] # second 2 stims are word\n",
    "\n",
    "beta_dict = {}\n",
    "\n",
    "# Organize subject betas by stimulus\n",
    "for subj in subjNums:\n",
    "    print 'Loading betas for subject', subj\n",
    "    betas = loadBetas(subj)\n",
    "    i=0\n",
    "    beta_dict[subj] = np.zeros((betas.shape[0],ntrials_perstim,nstims))\n",
    "    for stim in range(nstims):\n",
    "        for trial in range(ntrials_perstim):\n",
    "            beta_dict[subj][:,trial,stim] = betas[:,i]\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 2 classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runCV(svmmat, labels, cvs=10, leaveout=5, replacement=False):\n",
    "    \"\"\"\n",
    "    Runs a cross validation given an svm matrix, labels, number of cross validations, and \n",
    "    number of leave outs for test predictions\n",
    "    \n",
    "    Assumes cross validation without replacement (for each train v. test prediction)\n",
    "    \"\"\"\n",
    "    # Get number of classes\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    # Run cross-validation using Linear SVMs\n",
    "    leaveout_total = leaveout*len(classes)\n",
    "\n",
    "    # Spatially demean (across features, to ensure that mean activity does not factor into classification)\n",
    "    spatialmean = np.mean(svmmat,axis=1)\n",
    "    spatialmean.shape = (len(spatialmean),1)\n",
    "    svmmat = svmmat - spatialmean\n",
    "    \n",
    "    # Start cross-validation\n",
    "    nsamples = svmmat.shape[0]\n",
    "    acc = []\n",
    "    for cv in range(cvs):\n",
    "        test_ind = []\n",
    "        for stim in classes:\n",
    "            stimlabs = np.where(labels==stim)[0]\n",
    "            test_ind.extend(np.random.choice(stimlabs,leaveout,replace=replacement))\n",
    "            \n",
    "        test_ind = np.asarray(test_ind)\n",
    "        train_ind = np.delete(np.arange(nsamples),test_ind)\n",
    "\n",
    "        trainset = svmmat[train_ind,:]\n",
    "        testset = svmmat[test_ind,:]\n",
    "        # normalize train and test set according to train set mean & std\n",
    "        mean = np.mean(trainset,axis=0)\n",
    "        std = np.std(trainset,axis=0)\n",
    "        trainset = np.divide((trainset - mean),std)\n",
    "        testset = np.divide((testset - mean),std)\n",
    "\n",
    "        # Construct classifier and fit\n",
    "        svc = SVC(kernel='linear')\n",
    "        svc.fit(trainset,labels[train_ind])\n",
    "        # Get acc\n",
    "        acc.append(svc.score(testset,labels[test_ind]))\n",
    "\n",
    "    # Get average accuracy\n",
    "#     accmat[subj][roi_ind,0] = np.mean(acc)\n",
    "    avg_acc = np.mean(acc)\n",
    "\n",
    "    \n",
    "    # Perform within-subject binomial test\n",
    "    ntotal = leaveout_total*cvs\n",
    "    nsuccess = avg_acc*ntotal\n",
    "    chance = 1.0/len(classes)\n",
    "    p = stats.binom_test(nsuccess,ntotal,p=chance)\n",
    "\n",
    "    # Make sure it's a one-sided binomial test\n",
    "    if avg_acc > chance:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0 - p/2.0\n",
    "    \n",
    "    return avg_acc, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run actual code/cross-validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifications for subject 101\n",
      "Running classifications for subject 102\n"
     ]
    }
   ],
   "source": [
    "accmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "accmat_thresh = {}\n",
    "nparcels = 360\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running classifications for subject', subj\n",
    "    accmat[subj] = np.zeros((glasser.shape[0],3)) # 2 for two classifications; 3rd is for 4-way class    \n",
    "    pmat[subj] = np.zeros((nparcels,3))\n",
    "    \n",
    "    for roi in range(nparcels):\n",
    "        if nparcels%100==0:\n",
    "            print 'Running classifications on ROI,', roi + 1\n",
    "        roi_ind = np.where(glasser==roi+1)[0]\n",
    "        \n",
    "        ####\n",
    "        ## Color STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        mat_tmp = [] # For a 2-way classification\n",
    "        labels_tmp = [] \n",
    "        for stim in colorstims:\n",
    "            mat_tmp.append(beta_dict[subj][roi_ind,:,stim].T)\n",
    "            labels_tmp.append(np.ones((ntrials_perstim,1))*stim)\n",
    "        svmmat_color = np.vstack(mat_tmp)\n",
    "        labels_color = np.squeeze(np.vstack(labels_tmp))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_color, labels_color, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,0] = acc\n",
    "        pmat[subj][roi,0] = p\n",
    "        h0, qs = mc.fdrcorrection0(pmat[subj][:,0])\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## Word STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        mat_tmp = [] # For a 2-way classification\n",
    "        labels_tmp = [] \n",
    "        for stim in wordstims:\n",
    "            mat_tmp.append(beta_dict[subj][roi_ind,:,stim].T)\n",
    "            labels_tmp.append(np.ones((ntrials_perstim,1))*stim)\n",
    "        svmmat_word = np.vstack(mat_tmp)\n",
    "        labels_word = np.squeeze(np.vstack(labels_tmp))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_word, labels_word, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,1] = acc\n",
    "        pmat[subj][roi,1] = p\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## 4-way STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        svmmat_4way = np.vstack((svmmat_color,svmmat_word))\n",
    "        labels_4way = np.squeeze(np.hstack((labels_color,labels_word)))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_4way, labels_4way, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,2] = acc\n",
    "        pmat[subj][roi,2] = p\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34\n",
      "  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34\n",
      "  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34\n",
      "  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34\n",
      "  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34  0.34]\n",
      "[ 0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55\n",
      "  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55\n",
      "  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55\n",
      "  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55\n",
      "  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55]\n"
     ]
    }
   ],
   "source": [
    "print accmat['101'][roi_ind,2]\n",
    "print accmat['102'][roi_ind,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run FDR correction for each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qmat = {}\n",
    "accmat_thresh = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    accmat_thresh[subj] = np.zeros(accmat[subj].shape)\n",
    "    \n",
    "    for test in range(accmat[subj].shape[1]):\n",
    "        h0,qs = mc.fdrcorrection0(pmat[subj][:,test])\n",
    "        \n",
    "        # Iterate through each ROI and make sure it is significant\n",
    "        for roi in range(nparcels):\n",
    "            roi_ind = np.where(glasser==roi+1)[0]\n",
    "            if qs[roi] < 0.05:\n",
    "                accmat_thresh[subj][roi_ind,test] = accmat[subj][roi_ind,test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_mvpa = basedir + subj + '_neutralStimuliMVPA_Acc.csv'\n",
    "    filename_mvpafdr = basedir + subj + '_neutralStimuliMVPA_AccThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_mvpa, accmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_mvpafdr, accmat_thresh[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "\n",
    "    \n",
    "    cifti_mvpa = basedir + subj + '_neutralStimuliMVPA_Acc.dscalar.nii'\n",
    "    cifti_mvpafdr = basedir + subj + '_neutralStimuliMVPA_AccThresh.dscalar.nii'\n",
    "    \n",
    "    filename_mvpa = basedir + subj + '_neutralStimuliMVPA_Acc.csv'\n",
    "    filename_mvpafdr = basedir + subj + '_neutralStimuliMVPA_AccThresh.csv'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_mvpa, cifti_mvpa)\n",
    "    convertCSVToCIFTI64k(filename_mvpafdr, cifti_mvpafdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run statistical analysis (i.e., activation threshold analysis) with contrasts against other stimulus conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 45\n",
    "nstims = 4\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalar(subj)\n",
    "    betas = stats.zscore(betas,axis=0)    \n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(4,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(a,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "        beta_mat_contrast = np.zeros((betas.shape[0],ntrials_perstim*(nstims-1)))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "        \n",
    "        notcond_ind = np.where(stim_ind!=stim)[0]\n",
    "        beta_mat_contrast[:,:] = betas[:,notcond_ind]        \n",
    "        \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_ind(beta_mat[vertex,:],beta_mat_contrast[vertex,:])\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim]) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Run task-rule encoding against 0\n",
    "### Isolate top-down control (i.e., hypothesized PFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBetasFromDscalarTaskRuleEncoding(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "    # Load smoothed betas!\n",
    "    betas = nib.load(datadir + subj + '_taskRuleEncoding_taskBetas_Surface64k_sm.dscalar.nii')\n",
    "    betas = betas.get_data()\n",
    "    betas = np.squeeze(betas)\n",
    "    betas = betas.T\n",
    "    betas = betas[:,18:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 101\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n",
      "Subject 102\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n"
     ]
    }
   ],
   "source": [
    "for subj in subjNums:\n",
    "    print 'Subject', subj\n",
    "    glmdir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/' \n",
    "    \n",
    "    print 'Converting CSV to dscalar...'\n",
    "    csvfile = glmdir + subj + '_taskruleEncoding_taskBetas_Surface64k.csv'\n",
    "    dscalarfile = glmdir + subj + '_taskRuleEncoding_taskBetas_Surface64k.dscalar.nii'\n",
    "    convertCSVToCIFTI64k(csvfile, dscalarfile)\n",
    "    \n",
    "    print 'Smoothing surface files...'\n",
    "    dscalar_sm_file = glmdir + subj + '_taskRuleEncoding_taskBetas_Surface64k_sm.dscalar.nii'\n",
    "    subjdir = '/projects3/StroopActFlow/data/' + subj + '/MNINonLinear/fsaverage_LR32k/'\n",
    "    lsurf = subjdir + subj + '.L.inflated.32k_fs_LR.surf.gii'\n",
    "    rsurf = subjdir + subj + '.R.inflated.32k_fs_LR.surf.gii'\n",
    "    \n",
    "    wb_command = 'wb_command -cifti-smoothing ' + dscalarfile + ' 4 4 COLUMN ' + dscalar_sm_file\n",
    "    wb_command += ' -left-surface ' + lsurf\n",
    "    wb_command += ' -right-surface ' + rsurf\n",
    "    os.system(wb_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.1 Run statistical analysis (i.e., activation threshold analysis) against 0\n",
    "* Task 1: ColorEncoding\n",
    "* Task 2: WordEncoding\n",
    "\n",
    "Betas are z-scored prior to running t-tests against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 30\n",
    "nstims = 2\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalarTaskRuleEncoding(subj)\n",
    "    betas = stats.zscore(betas,axis=0)\n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims + 1)) # Include conjunction\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(nstims,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(stim_ind,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "            \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_1samp(beta_mat[vertex,:],0)\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim])         \n",
    "        # Include conjunction map\n",
    "        tmat_fdr[subj][:,2] = tmat_fdr[subj][:,2] + h0\n",
    "        \n",
    "    tmat_fdr[subj][:,2] = tmat_fdr[subj][:,2] == nstims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_taskRuleEncoding_tstats.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Run task-rule encoding contrasts (color v. word)\n",
    "### Isolate top-down control (i.e., hypothesized PFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Run statistical analysis (i.e., activation threshold analysis) against 0\n",
    "* Task 1: ColorEncoding\n",
    "* Task 2: WordEncoding\n",
    "\n",
    "Betas are z-scored prior to running t-tests against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 30\n",
    "nstims = 2\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalarTaskRuleEncoding(subj)\n",
    "    betas = stats.zscore(betas,axis=0)\n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims)) # Include conjunction\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(nstims,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(stim_ind,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "        beta_mat_contrast = np.zeros((betas.shape[0],ntrials_perstim*(nstims-1)))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "      \n",
    "        notcond_ind = np.where(stim_ind!=stim)[0]\n",
    "        beta_mat_contrast[:,:] = betas[:,notcond_ind]        \n",
    "      \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_rel(beta_mat[vertex,:],beta_mat_contrast[vertex,:])\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim])         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "* No significant voxels for word v. color rule contrasts (within subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
