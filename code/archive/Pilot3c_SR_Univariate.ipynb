{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot 2 -- Compute the univariate activations of various SR stim-context integrations\n",
    "### Ex: Color stims ([red, red] v. [red, blue] v. [blue, red] v. [blue, blue])\n",
    "\n",
    "## Use SVM classifications to input stimuli (both modalities: visual and auditory)\n",
    "## Using Ciric-style postprocessing\n",
    "\n",
    "## Takuya Ito\n",
    "#### 12/06/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as stats\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import statsmodels.api as sm\n",
    "import sklearn.svm as svm\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import sklearn\n",
    "from sklearn.feature_selection import f_classif\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "os.sys.path.append('glmScripts/')\n",
    "import taskGLMPipeline as tgp\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"FreeSans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 084\n",
    "subjNums = ['013','014','016','017','018','021','023','024','026','027','028','030','031','032','033',\n",
    "            '034','035','037','038','039','040','041','042','043','045','046','047','048','049','050',\n",
    "            '053','055','056','057','058','062','063','066','067','068','069','070','072','074','075',\n",
    "            '076','077','081','085','086','087','088','090','092','093','094','095','097','098','099',\n",
    "            '101','102','103','104','105','106','108','109','110','111','112','114','115','117','119',\n",
    "            '120','121','122','123','124','125','126','127','128','129','130','131','132','134','135',\n",
    "            '136','137','138','139','140','141']\n",
    "\n",
    "\n",
    "\n",
    "basedir = '/projects3/SRActFlow/'\n",
    "\n",
    "# Using final partition\n",
    "networkdef = np.loadtxt('/projects3/NetworkDiversity/data/network_partition.txt')\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "# network mappings for final partition set\n",
    "networkmappings = {'fpn':7, 'vis1':1, 'vis2':2, 'smn':3, 'aud':8, 'lan':6, 'dan':5, 'con':4, 'dmn':9, \n",
    "                   'pmulti':10, 'none1':11, 'none2':12}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "\n",
    "## General parameters/variables\n",
    "nParcels = 360\n",
    "nSubjs = len(subjNums)\n",
    "\n",
    "glasserfile2 = '/projects/AnalysisTools/ParcelsGlasser2016/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_RL.dlabel.nii'\n",
    "glasser2 = nib.load(glasserfile2).get_data()\n",
    "glasser2 = np.squeeze(glasser2)\n",
    "\n",
    "sortednets = np.sort(xticks.keys())\n",
    "orderednetworks = []\n",
    "for net in sortednets: orderednetworks.append(xticks[net])\n",
    "    \n",
    "networkpalette = ['royalblue','slateblue','paleturquoise','darkorchid','limegreen',\n",
    "                  'lightseagreen','yellow','orchid','r','peru','orange','olivedrab']\n",
    "networkpalette = np.asarray(networkpalette)\n",
    "\n",
    "OrderedNetworks = ['VIS1','VIS2','SMN','CON','DAN','LAN','FPN','AUD','DMN','PMM','VMM','ORA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Define functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputActivity(subj,inputtype):\n",
    "    x = tgp.loadTaskTiming('013','ALL')\n",
    "    stimIndex = np.asarray(x['stimIndex'])\n",
    "    ind = np.where(stimIndex==inputtype)[0]+1\n",
    "    \n",
    "    datadir = basedir + 'data/postProcessing/hcpPostProcCiric/'\n",
    "    h5f = h5py.File(datadir + subj + '_glmOutput_data.h5','r')\n",
    "    data = h5f['taskRegression/ALL_24pXaCompCorXVolterra_taskReg_betas_canonical'][:].copy()\n",
    "    data = data[:,ind].copy()\n",
    "    h5f.close()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Run across subject univariate analysis on RED SR rules\n",
    "### 4-way decoding\n",
    "* both red\n",
    "* notboth red\n",
    "* either red\n",
    "* neither red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data for RED SR integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nStims = 4\n",
    "data_task = np.zeros((len(glasser2),nStims,len(subjNums)))\n",
    "sr_rule = 'srRed'\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task[:,:,scount] = loadInputActivity(subj,sr_rule)\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perform univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform\n",
    "\n",
    "group_stats = np.zeros((data_task.shape[0],data_task.shape[1])) #t, p, t_thresh\n",
    "for stim in range(data_task.shape[1]):\n",
    "    t, p = stats.ttest_1samp(data_task[:,stim,:],0,axis=1)\n",
    "    h0, qs = mc.fdrcorrection0(p)\n",
    "    group_stats[:,stim] = np.multiply(t,h0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Map activations back to cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#### \n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_SR/'\n",
    "filename = sr_rule + '_SR_Univariate'\n",
    "np.savetxt(outdir + filename + '.csv', group_stats,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Run across subject univariate on VERTICAL SR rules\n",
    "### 4-way decoding\n",
    "* both vertical\n",
    "* notboth vertical\n",
    "* either vertical\n",
    "* neither vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data for VERTICAL SR integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nStims = 4\n",
    "data_task = np.zeros((len(glasser2),nStims,len(subjNums)))\n",
    "sr_rule = 'srVertical'\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task[:,:,scount] = loadInputActivity(subj,sr_rule)\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perform univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform\n",
    "\n",
    "group_stats = np.zeros((data_task.shape[0],data_task.shape[1])) #t, p, t_thresh\n",
    "for stim in range(data_task.shape[1]):\n",
    "    t, p = stats.ttest_1samp(data_task[:,stim,:],0,axis=1)\n",
    "    h0, qs = mc.fdrcorrection0(p)\n",
    "    group_stats[:,stim] = np.multiply(t,h0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Map activations back to cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#### \n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_SR/'\n",
    "filename = sr_rule + '_SR_Univariate'\n",
    "np.savetxt(outdir + filename + '.csv', group_stats,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Run across subject univariate analysis on HIGH SR rules\n",
    "### 4-way decoding\n",
    "* both high\n",
    "* notboth high\n",
    "* either high\n",
    "* neither high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data for HIGH SR integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nStims = 4\n",
    "data_task = np.zeros((len(glasser2),nStims,len(subjNums)))\n",
    "sr_rule = 'srHigh'\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task[:,:,scount] = loadInputActivity(subj,sr_rule)\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perform univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda2/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/usr/local/anaconda2/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/usr/local/anaconda2/lib/python2.7/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n",
      "/usr/local/anaconda2/lib/python2.7/site-packages/statsmodels/stats/multitest.py:320: RuntimeWarning: invalid value encountered in less_equal\n",
      "  reject = pvals_sorted <= ecdffactor*alpha\n"
     ]
    }
   ],
   "source": [
    "# Perform\n",
    "\n",
    "group_stats = np.zeros((data_task.shape[0],data_task.shape[1])) #t, p, t_thresh\n",
    "for stim in range(data_task.shape[1]):\n",
    "    t, p = stats.ttest_1samp(data_task[:,stim,:],0,axis=1)\n",
    "    h0, qs = mc.fdrcorrection0(p)\n",
    "    group_stats[:,stim] = np.multiply(t,h0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Map activations back to cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#### \n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_SR/'\n",
    "filename = sr_rule + '_SR_Univariate'\n",
    "np.savetxt(outdir + filename + '.csv', group_stats,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Run across subject univariate analysis on CONSTANT SR rules\n",
    "### 4-way decoding\n",
    "* both constant\n",
    "* notboth constant\n",
    "* either constant\n",
    "* neither constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data for CONSTANT SR integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nStims = 4\n",
    "data_task = np.zeros((len(glasser2),nStims,len(subjNums)))\n",
    "sr_rule = 'srConstant'\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task[:,:,scount] = loadInputActivity(subj,sr_rule)\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Perform univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform\n",
    "\n",
    "group_stats = np.zeros((data_task.shape[0],data_task.shape[1])) #t, p, t_thresh\n",
    "for stim in range(data_task.shape[1]):\n",
    "    t, p = stats.ttest_1samp(data_task[:,stim,:],0,axis=1)\n",
    "    h0, qs = mc.fdrcorrection0(p)\n",
    "    group_stats[:,stim] = np.multiply(t,h0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Map activations back to cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#### \n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_SR/'\n",
    "filename = sr_rule + '_SR_Univariate'\n",
    "np.savetxt(outdir + filename + '.csv', group_stats,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
