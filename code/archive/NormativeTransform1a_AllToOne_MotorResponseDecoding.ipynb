{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ActFlow 1 -- Compute the baseline decodability of Motor responses (LINDEX v. LMID and RINDEX v. RMID)\n",
    "## Using ActFlow, all to one, via ridge FC\n",
    "\n",
    "## Use SVM classifications to decode hand-specific responses\n",
    "## Using Ciric-style postprocessing\n",
    "\n",
    "## Takuya Ito\n",
    "#### 12/12/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as stats\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import statsmodels.api as sm\n",
    "import sklearn.svm as svm\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import sklearn\n",
    "from sklearn.feature_selection import f_classif\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "os.sys.path.append('glmScripts/')\n",
    "import taskGLMPipeline as tgp\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"FreeSans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 084\n",
    "subjNums = ['013','014','016','017','018','021','023','024','026','027','028','030','031','032','033',\n",
    "            '034','035','037','038','039','040','041','042','043','045','046','047','048','049','050',\n",
    "            '053','055','056','057','058','062','063','066','067','068','069','070','072','074','075',\n",
    "            '076','077','081','085','086','087','088','090','092','093','094','095','097','098','099',\n",
    "            '101','102','103','104','105','106','108','109','110','111','112','114','115','117','119',\n",
    "            '120','121','122','123','124','125','126','127','128','129','130','131','132','134','135',\n",
    "            '136','137','138','139','140','141']\n",
    "\n",
    "\n",
    "\n",
    "basedir = '/projects3/SRActFlow/'\n",
    "\n",
    "# Using final partition\n",
    "networkdef = np.loadtxt('/projects3/NetworkDiversity/data/network_partition.txt')\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "# network mappings for final partition set\n",
    "networkmappings = {'fpn':7, 'vis1':1, 'vis2':2, 'smn':3, 'aud':8, 'lan':6, 'dan':5, 'con':4, 'dmn':9, \n",
    "                   'pmulti':10, 'none1':11, 'none2':12}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "\n",
    "## General parameters/variables\n",
    "nParcels = 360\n",
    "nSubjs = len(subjNums)\n",
    "\n",
    "glasserfile2 = '/projects/AnalysisTools/ParcelsGlasser2016/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_RL.dlabel.nii'\n",
    "glasser2 = nib.load(glasserfile2).get_data()\n",
    "glasser2 = np.squeeze(glasser2)\n",
    "\n",
    "sortednets = np.sort(xticks.keys())\n",
    "orderednetworks = []\n",
    "for net in sortednets: orderednetworks.append(xticks[net])\n",
    "    \n",
    "networkpalette = ['royalblue','slateblue','paleturquoise','darkorchid','limegreen',\n",
    "                  'lightseagreen','yellow','orchid','r','peru','orange','olivedrab']\n",
    "networkpalette = np.asarray(networkpalette)\n",
    "\n",
    "OrderedNetworks = ['VIS1','VIS2','SMN','CON','DAN','LAN','FPN','AUD','DMN','PMM','VMM','ORA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Define functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMotorResponses(subj,hand='Right'):\n",
    "    \n",
    "    hands = {'Left':[0,1],'Right':[2,3]}\n",
    "\n",
    "    x = tgp.loadTaskTiming(subj,'ALL')\n",
    "    stimIndex = np.asarray(x['stimIndex'])\n",
    "    ind = np.where(stimIndex=='motorResponse')[0]\n",
    "    \n",
    "    datadir = basedir + 'data/postProcessing/hcpPostProcCiric/'\n",
    "    h5f = h5py.File(datadir + subj + '_glmOutput_data.h5','r')\n",
    "    data = h5f['taskRegression/ALL_24pXaCompCorXVolterra_taskReg_betas_canonical'][:].copy()\n",
    "    data = data[:,ind].copy()\n",
    "    h5f.close()\n",
    "    \n",
    "    # Isolate hand responses\n",
    "    hand_ind = hands[hand]\n",
    "    data = data[:,hand_ind]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def loadRSFCMapping(subj,roi):\n",
    "    fcdir = '/projects3/SRActFlow/data/results/ridgeFC/'\n",
    "    filename = fcdir + 'TargetParcel' + str(roi) + '_RidgeFC.h5'\n",
    "    h5f = h5py.File(filename,'r')\n",
    "    fcmapping = h5f[subj]['sourceToTargetMapping'][:].copy()\n",
    "    h5f.close()\n",
    "    return fcmapping\n",
    "\n",
    "def ridgeWrapper((stim,resp,alpha)):\n",
    "#     wt = ridge.ridge.ridge(stim,resp,alpha)\n",
    "    clf = Ridge(alpha=alpha)\n",
    "    clf.fit(stim,resp)\n",
    "    wt = clf.coef_\n",
    "\n",
    "    return wt\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nResponses = 2\n",
    "data_task_rh = np.zeros((len(glasser2),nResponses,len(subjNums)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task_rh[:,:,scount] = loadMotorResponses(subj, hand='Right')\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Generate normative transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load masks\n",
    "def loadMask(roi,dilated=True):\n",
    "    maskdir = basedir + 'data/results/surfaceMasks/'\n",
    "    if dilated:\n",
    "        maskfile = maskdir + 'GlasserParcel' + str(roi) + '_dilated_10mm.dscalar.nii'\n",
    "    else:\n",
    "        maskfile = maskdir + 'GlasserParcel' + str(roi) + '.dscalar.nii'\n",
    "    maskdata = np.squeeze(nib.load(maskfile).get_data())\n",
    "    return maskdata\n",
    "\n",
    "def loadRegularizationTerms(roi):\n",
    "    ridgefcdir = '/projects3/SRActFlow/data/results/ridgeFC/'\n",
    "    filename = ridgefcdir + 'TargetParcel' + str(roi) + '_RidgeFC.h5'\n",
    "    h5f = h5py.File(filename,'r')\n",
    "    alphas = h5f['alphasPerVertex'][:].copy()\n",
    "    return alphas\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OMP_NUM_THREADS'] = str(20)\n",
    "\n",
    "# roi_lh = 9\n",
    "# roi_rh = 189\n",
    "# actflow_data = np.zeros((len(glasser2),nResponses,len(subjNums),nParcels))\n",
    "\n",
    "# dilateLH = loadMask(roi_lh,dilated=True)\n",
    "# dilateRH = loadMask(roi_rh,dilated=True)\n",
    "# combinedDilated = dilateLH + dilateRH\n",
    "# source_space = np.where(combinedDilated==0)[0]\n",
    "\n",
    "# # alphas_lh = loadRegularizationTerms(roi_lh)\n",
    "# # alphas_rh = loadRegularizationTerms(roi_rh)\n",
    "# alphas_lh = 10\n",
    "# alphas_rh = 10\n",
    "\n",
    "# scount = 0\n",
    "# for subj in subjNums:\n",
    "#     print 'Subject', subj, '(', scount+1, '/', len(subjNums), ')'\n",
    "#     training_subjs = np.delete(np.arange(len(subjNums)),scount)\n",
    "    \n",
    "#     for roi in range(nParcels):\n",
    "#         if roi%30==0: print '\\tROI number', roi+1\n",
    "#         source_ind = np.where(glasser2==roi+1)[0]\n",
    "#         source_ind = np.intersect1d(source_ind,source_space) # Make sure no vertices are within 10mm of the target\n",
    "#         if source_ind.shape[0]==0:\n",
    "#             print 'This source has no vertices, skipping'\n",
    "#             continue\n",
    "#         if roi%30==0: print '\\tFinding normative mapping RH S1'\n",
    "#         target_ind = np.where(glasser2==roi_rh)[0]\n",
    "#         targetTrain = data_task[:,:,training_subjs].T.reshape(len(training_subjs)*4,len(glasser2))[:,target_ind]\n",
    "#         sourceTrain = data_task[:,:,training_subjs].T.reshape(len(training_subjs)*4,len(glasser2))[:,source_ind]\n",
    "#         normativeMapping = ridgeWrapper((sourceTrain,targetTrain,alphas_rh))\n",
    "#         normativeMapping = normativeMapping.T\n",
    "        \n",
    "        \n",
    "\n",
    "#         # Left Finger 1\n",
    "#         actflow_data[target_ind,0,scount,roi] = np.dot(stats.zscore(data_task[source_ind,0,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Left Finger 2\n",
    "#         actflow_data[target_ind,1,scount,roi] = np.dot(stats.zscore(data_task[source_ind,1,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Right Finger 1\n",
    "#         actflow_data[target_ind,2,scount,roi] = np.dot(stats.zscore(data_task[source_ind,2,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Right Finger 2\n",
    "#         actflow_data[target_ind,3,scount,roi] = np.dot(stats.zscore(data_task[source_ind,3,scount],axis=0),normativeMapping[:,:])\n",
    "\n",
    "#         if roi%30==0: print '\\tFinding normative mapping LH S1'\n",
    "#         target_ind = np.where(glasser2==roi_lh)[0]\n",
    "#         targetTrain = data_task[:,:,training_subjs].T.reshape(len(training_subjs)*4,len(glasser2))[:,target_ind]\n",
    "#         sourceTrain = data_task[:,:,training_subjs].T.reshape(len(training_subjs)*4,len(glasser2))[:,source_ind]\n",
    "#         normativeMapping = ridgeWrapper((sourceTrain,targetTrain,alphas_lh))\n",
    "#         normativeMapping = normativeMapping.T\n",
    "\n",
    "#         # Left Finger 1\n",
    "#         actflow_data[target_ind,0,scount,roi] = np.dot(stats.zscore(data_task[source_ind,0,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Left Finger 2\n",
    "#         actflow_data[target_ind,1,scount,roi] = np.dot(stats.zscore(data_task[source_ind,1,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Right Finger 1\n",
    "#         actflow_data[target_ind,2,scount,roi] = np.dot(stats.zscore(data_task[source_ind,2,scount],axis=0),normativeMapping[:,:])\n",
    "#         # Right Finger 2\n",
    "#         actflow_data[target_ind,3,scount,roi] = np.dot(stats.zscore(data_task[source_ind,3,scount],axis=0),normativeMapping[:,:])\n",
    "    \n",
    "#     scount += 1\n",
    "# \n",
    "# outdir = '/projects3/SRActFlow/data/results/NormativeTransform1a_RidgeAllToOne/'\n",
    "# h5f_rh = h5py.File(outdir + 'ActFlowData_Target' + str(roi_rh) + '.h5','a')\n",
    "# h5f_rh.create_dataset('data',data=actflow_data_rh)\n",
    "# h5f_rh.close()\n",
    "\n",
    "# h5f_lh = h5py.File(outdir + 'ActFlowData_Target' + str(roi_lh) + '.h5','a')\n",
    "# h5f_lh.create_dataset('data',data=actflow_data_lh)\n",
    "# h5f_lh.close()\n",
    "\n",
    "roi_lh = 9\n",
    "roi_rh = 189\n",
    "\n",
    "outdir = '/projects3/SRActFlow/data/results/NormativeTransform1a_RidgeAllToOne/'\n",
    "## Load existing data\n",
    "h5f_rh = h5py.File(outdir + 'ActFlowData_Target' + str(roi_rh) + '.h5','r')\n",
    "actflow_data_rh = h5f_rh['data'][:].copy()\n",
    "h5f_rh.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define functions for motor response decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def motorResponseDecodings(data, actflow_data, rois, ncvs=1, nproc=5):\n",
    "    \"\"\"\n",
    "    Run an across-subject classification\n",
    "    Decode responses on each hand separately from CPRO data\n",
    "    \"\"\"\n",
    "\n",
    "    nSubjs = data.shape[2]\n",
    "    stats = np.zeros((len(rois),))\n",
    "    \n",
    "    nfing = data.shape[1]\n",
    "\n",
    "    nsamples = nSubjs * nfing\n",
    "\n",
    "    # Label array for supervised learning\n",
    "    labels = np.tile(range(nfing),nSubjs)\n",
    "    subjarray = np.repeat(range(nSubjs),nfing)\n",
    "\n",
    "    # Run SVM classifications on network-level activation patterns across subjects\n",
    "    roicount = 0\n",
    "    for roi in rois:\n",
    "        roi_ind = np.where(glasser2==roi)[0]\n",
    "        nfeatures = len(roi_ind)\n",
    "        roi_ind.shape = (len(roi_ind),1)       \n",
    "\n",
    "        svm_mat = np.zeros((nsamples,roi_ind.shape[0]))\n",
    "        actflow_svm_mat = np.zeros((nsamples,roi_ind.shape[0]))\n",
    "        samplecount = 0\n",
    "        scount = 0\n",
    "        for subj in range(len(subjNums)):\n",
    "            roidata = np.squeeze(data[roi_ind,:,scount])\n",
    "            actflow_roidata = np.squeeze(actflow_data[roi_ind,:,scount])\n",
    "            svm_mat[samplecount:(samplecount+nfing),:] = roidata.T\n",
    "            actflow_svm_mat[samplecount:(samplecount+nfing),:] = actflow_roidata.T\n",
    "\n",
    "            scount += 1\n",
    "            samplecount += nfing\n",
    "\n",
    "            # Spatially demean matrix across features\n",
    "            samplemean = np.mean(svm_mat,axis=1)\n",
    "            samplemean.shape = (len(samplemean),1)\n",
    "            svm_mat = svm_mat - samplemean\n",
    "            \n",
    "            samplemean = np.mean(actflow_svm_mat,axis=1)\n",
    "            samplemean.shape = (len(samplemean),1)\n",
    "            actflow_svm_mat = actflow_svm_mat - samplemean\n",
    "\n",
    "        scores = randomSplitLOOBaselineCV(ncvs, svm_mat, actflow_svm_mat, labels, subjarray, nproc=nproc)\n",
    "        stats[roicount] = np.mean(scores)\n",
    "        roicount += 1\n",
    "        \n",
    "    return stats\n",
    "\n",
    "def randomSplitLOOBaselineCV(ncvs, svm_mat, actflow_svm_mat, labels, subjarray, nproc=5):\n",
    "    \"\"\"\n",
    "    Runs cross validation for an across-subject SVM analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    ntasks = len(np.unique(labels))\n",
    "    nsamples = svm_mat.shape[0]\n",
    "    nsubjs = nsamples/ntasks\n",
    "\n",
    "    subjects = np.unique(subjarray)\n",
    "    indices = np.arange(nsamples)\n",
    "    \n",
    "    numsubjs_perfold = 16\n",
    "    if nsubjs%numsubjs_perfold!=0: \n",
    "        raise Exception(\"Error: Folds don't match number of subjects\")\n",
    "        \n",
    "    nfolds = nsubjs/numsubjs_perfold\n",
    "    subj_array_folds = subjarray.copy()\n",
    "    \n",
    "    inputs = [] \n",
    "    \n",
    "    for fold in range(nfolds):\n",
    "        test_subjs = np.random.choice(subj_array_folds,numsubjs_perfold,replace=False)\n",
    "        train_subjs_all = np.delete(subjects,test_subjs)\n",
    "        for cv in range(ncvs):\n",
    "            # Randomly sample half of train set subjects for each cv (CV bootstrapping)\n",
    "            train_subjs = np.random.choice(train_subjs_all,\n",
    "                                         int(np.floor(len(train_subjs_all)*(4.0))),\n",
    "                                         replace=True)\n",
    "\n",
    "            train_ind = []\n",
    "            for subj in train_subjs:\n",
    "                train_ind.extend(np.where(subjarray==subj)[0])\n",
    "\n",
    "            test_ind = []\n",
    "            for subj in test_subjs:\n",
    "                test_ind.extend(np.where(subjarray==subj)[0])\n",
    "            \n",
    "            train_ind = np.asarray(train_ind)\n",
    "            test_ind = np.asarray(test_ind)\n",
    "\n",
    "            trainset = actflow_svm_mat[train_ind,:]\n",
    "            testset = svm_mat[test_ind,:]\n",
    "            orig_training = svm_mat[train_ind,:]\n",
    "\n",
    "#             # Normalize trainset and testset\n",
    "#             trainmean = np.mean(actflow_svm_mat[train_ind,:],axis=0)\n",
    "#             trainmean.shape = (1,len(trainmean))\n",
    "#             trainstd = np.std(actflow_svm_mat[train_ind,:],axis=0)\n",
    "#             trainstd.shape = (1,len(trainstd))\n",
    "            \n",
    "#             # Normalize trainset and testset\n",
    "#             testmean = np.mean(svm_mat[train_ind,:],axis=0)\n",
    "#             testmean.shape = (1,len(testmean))\n",
    "#             teststd = np.std(svm_mat[train_ind,:],axis=0)\n",
    "#             teststd.shape = (1,len(teststd))\n",
    "\n",
    "#             trainset = np.divide((trainset - trainmean),trainstd)\n",
    "#             testset = np.divide((testset - testmean),teststd)\n",
    "\n",
    "            ######## FEATURE SELECTION & REDUCTION\n",
    "            ## Feature selection and downsampling\n",
    "            trainlabels = labels[train_ind]\n",
    "            testlabels = labels[test_ind]\n",
    "            unique_labels = np.unique(labels)\n",
    "            feat1_labs = np.where(trainlabels==0)[0]\n",
    "            feat2_labs = np.where(trainlabels==1)[0]\n",
    "            # Perform t-test\n",
    "            t, p = stats.ttest_rel(orig_training[feat1_labs,:],orig_training[feat2_labs,:],axis=0)\n",
    "            h0, qs = mc.fdrcorrection0(p)\n",
    "#             h0 = p<0.1\n",
    "            # Construct feature masks\n",
    "            feat1_mask = np.multiply(t<0,h0)\n",
    "            feat2_mask = np.multiply(t>0,h0)\n",
    "            feat1_mask = t>0\n",
    "            feat2_mask = t<0\n",
    "            # Downsample training set into original vertices into 2 ROI signals\n",
    "            trainset_downsampled = np.zeros((trainset.shape[0],2))\n",
    "            trainset_downsampled[:,0] = np.nanmean(trainset[:,feat1_mask],axis=1)\n",
    "            trainset_downsampled[:,1] = np.nanmean(trainset[:,feat2_mask],axis=1)\n",
    "            # Downsample test set into original vertices\n",
    "            testset_downsampled = np.zeros((testset.shape[0],2))\n",
    "            testset_downsampled[:,0] = np.nanmean(testset[:,feat1_mask],axis=1)\n",
    "            testset_downsampled[:,1] = np.nanmean(testset[:,feat2_mask],axis=1)\n",
    "            \n",
    "#             print 'feat1_mask', np.sum(feat1_mask), '| feat2_mask', np.sum(feat2_mask)\n",
    "\n",
    "            if np.sum(feat1_mask)==0 or np.sum(feat2_mask)==0:\n",
    "                print 'not running feature selection'\n",
    "                inputs.append((trainset,testset,labels[train_ind],labels[test_ind]))\n",
    "            else:\n",
    "                inputs.append((trainset_downsampled,testset_downsampled,labels[train_ind],labels[test_ind]))\n",
    "\n",
    "#             inputs.append((trainset,testset,labels[train_ind],labels[test_ind]))         \n",
    "        subj_array_folds = np.delete(subj_array_folds,test_subjs)\n",
    "        \n",
    "    pool = mp.Pool(processes=nproc)\n",
    "    scores = pool.map_async(_decoding,inputs).get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    acc = []\n",
    "    for score in scores:\n",
    "        acc.extend(score)\n",
    "    return acc\n",
    "\n",
    "def _decoding((trainset,testset,trainlabels,testlabels)):\n",
    "\n",
    "#     clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf = svm.SVC(C=1.0, kernel='linear')\n",
    "\n",
    "    clf.fit(trainset,trainlabels)\n",
    "    predictions = clf.predict(testset)\n",
    "    acc = predictions==testlabels\n",
    "    \n",
    "#     unique_cond = np.unique(trainlabels)\n",
    "#     rdm = np.zeros((len(unique_cond),len(unique_cond)))\n",
    "#     acc = []\n",
    "#     for cond1 in unique_cond:\n",
    "#         mismatches = []\n",
    "#         prototype_ind = np.where(trainlabels==cond1)[0]\n",
    "#         prototype = np.mean(trainset[prototype_ind,:],axis=0)\n",
    "#         for cond2 in unique_cond:\n",
    "#             test_ind = np.where(testlabels==cond2)[0]\n",
    "#             test = np.mean(testset[test_ind,:],axis=0)\n",
    "#             if cond1 == cond2: \n",
    "#                 correct = stats.pearsonr(prototype,test)[0]\n",
    "#             else:\n",
    "#                 mismatches.append(stats.pearsonr(prototype,test)[0])\n",
    "        \n",
    "#         if correct > np.max(mismatches): \n",
    "#             acc.append(1.0)\n",
    "#         else:\n",
    "#             acc.append(0.0)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run across subject decoding on right-hand motor responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding roi 13\n",
      "Decoding roi 14\n",
      "Decoding roi 28\n",
      "Decoding roi 62\n",
      "Decoding roi 72\n",
      "Decoding roi 76\n",
      "Decoding roi 79\n",
      "Decoding roi 81\n",
      "Decoding roi 82\n",
      "Decoding roi 84\n",
      "Decoding roi 88\n",
      "Decoding roi 90\n",
      "Decoding roi 91\n",
      "Decoding roi 96\n",
      "Decoding roi 97\n",
      "Decoding roi 110\n",
      "Decoding roi 132\n",
      "Decoding roi 143\n",
      "Decoding roi 144\n",
      "Decoding roi 148\n",
      "Decoding roi 169\n",
      "Decoding roi 170\n",
      "Decoding roi 193\n",
      "Decoding roi 194\n",
      "Decoding roi 208\n",
      "Decoding roi 237\n",
      "Decoding roi 241\n",
      "Decoding roi 242\n",
      "Decoding roi 252\n",
      "Decoding roi 253\n",
      "Decoding roi 256\n",
      "Decoding roi 259\n",
      "Decoding roi 260\n",
      "Decoding roi 262\n",
      "Decoding roi 264\n",
      "Decoding roi 268\n",
      "Decoding roi 270\n",
      "Decoding roi 271\n",
      "Decoding roi 272\n",
      "Decoding roi 276\n",
      "Decoding roi 277\n",
      "Decoding roi 290\n",
      "Decoding roi 312\n",
      "Decoding roi 323\n",
      "Decoding roi 324\n",
      "Decoding roi 328\n",
      "Decoding roi 341\n",
      "Decoding roi 349\n",
      "Decoding roi 350\n",
      "Decoding roi 356\n",
      "Decoding roi 10\n",
      "Decoding roi 45\n",
      "Decoding roi 49\n",
      "Decoding roi 94\n",
      "Decoding roi 95\n",
      "Decoding roi 115\n",
      "Decoding roi 116\n",
      "Decoding roi 126\n",
      "Decoding roi 135\n",
      "Decoding roi 136\n",
      "Decoding roi 142\n",
      "Decoding roi 145\n",
      "Decoding roi 225\n",
      "Decoding roi 229\n",
      "Decoding roi 274\n",
      "Decoding roi 275\n",
      "Decoding roi 295\n",
      "Decoding roi 296\n",
      "Decoding roi 306\n",
      "Decoding roi 315\n",
      "Decoding roi 316\n",
      "Decoding roi 322\n",
      "Decoding roi 325\n",
      "Decoding roi 9\n",
      "Decoding roi 36\n",
      "Decoding roi 37\n",
      "Decoding roi 42\n",
      "Decoding roi 43\n",
      "Decoding roi 44\n",
      "Decoding roi 56\n",
      "Decoding roi 57\n",
      "Decoding roi 58\n",
      "Decoding roi 59\n",
      "Decoding roi 77\n",
      "Decoding roi 83\n",
      "Decoding roi 85\n",
      "Decoding roi 98\n",
      "Decoding roi 104\n",
      "Decoding roi 105\n",
      "Decoding roi 107\n",
      "Decoding roi 108\n",
      "Decoding roi 112\n",
      "Decoding roi 113\n",
      "Decoding roi 146\n",
      "Decoding roi 147\n",
      "Decoding roi 166\n",
      "Decoding roi 168\n",
      "Decoding roi 177\n",
      "Decoding roi 178\n",
      "Decoding roi 179\n",
      "Decoding roi 189\n",
      "Decoding roi 190\n",
      "Decoding roi 204\n",
      "Decoding roi 216\n",
      "Decoding roi 217\n",
      "Decoding roi 222\n",
      "Decoding roi 223\n",
      "Decoding roi 224\n",
      "Decoding roi 236\n",
      "Decoding roi 238\n",
      "Decoding roi 239\n",
      "Decoding roi 257\n",
      "Decoding roi 261\n",
      "Decoding roi 263\n",
      "Decoding roi 265\n",
      "Decoding roi 278\n",
      "Decoding roi 284\n",
      "Decoding roi 285\n",
      "Decoding roi 287\n",
      "Decoding roi 288\n",
      "Decoding roi 292\n",
      "Decoding roi 293\n",
      "Decoding roi 326\n",
      "Decoding roi 327\n",
      "Decoding roi 346\n",
      "Decoding roi 348\n",
      "Decoding roi 357\n",
      "Decoding roi 358\n",
      "Decoding roi 359\n"
     ]
    }
   ],
   "source": [
    "nproc = 20\n",
    "ncvs = 1\n",
    "\n",
    "rois = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "rois = [9] # Left S1\n",
    "\n",
    "\n",
    "sourceROIs = []\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "\n",
    "distances_baseline_rh = np.zeros((nParcels,len(subjNums)*2))\n",
    "for roi in sourceROIs:\n",
    "    print 'Decoding roi', roi\n",
    "#     if roi in bad_rois: \n",
    "#         continue\n",
    "\n",
    "    distances_baseline_rh[roi,:] = motorResponseDecodings(data_task_rh,\n",
    "                                                          actflow_data_rh[:,:,:,roi],\n",
    "                                                          rois=rois, ncvs=ncvs, nproc=nproc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant transfers: 19\n",
      "\tSignificant parcel: 37\n",
      "\tAccuracy: 0.609375\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 91\n",
      "\tAccuracy: 0.640625\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 117\n",
      "\tAccuracy: 0.59375\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 127\n",
      "\tAccuracy: 0.598958333333\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 133\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 179\n",
      "\tAccuracy: 0.598958333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 226\n",
      "\tAccuracy: 0.59375\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 237\n",
      "\tAccuracy: 0.619791666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 260\n",
      "\tAccuracy: 0.619791666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 261\n",
      "\tAccuracy: 0.609375\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 264\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 271\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 278\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 286\n",
      "\tAccuracy: 0.598958333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 288\n",
      "\tAccuracy: 0.640625\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 289\n",
      "\tAccuracy: 0.598958333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 316\n",
      "\tAccuracy: 0.625\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 326\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 347\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 4.0\n",
      "Network mappings: {'vis2': 2, 'lan': 6, 'vis1': 1, 'none2': 12, 'none1': 11, 'dan': 5, 'aud': 8, 'pmulti': 10, 'fpn': 7, 'dmn': 9, 'smn': 3, 'con': 4}\n"
     ]
    }
   ],
   "source": [
    "statistics_rh = np.zeros((distances_baseline_rh.shape[0],3))\n",
    "for roicount in range(distances_baseline_rh.shape[0]):\n",
    "    ntrials = len(subjNums)*2\n",
    "    p = stats.binom_test(np.mean(distances_baseline_rh[roicount,:])*ntrials,n=ntrials,p=0.5)\n",
    "    if np.mean(distances_baseline_rh[roicount,:])>0.5:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0-p/2.0\n",
    "\n",
    "    statistics_rh[roicount,0] = np.mean(distances_baseline_rh[roicount,:])\n",
    "    statistics_rh[roicount,1] = p\n",
    "\n",
    "rois_testing = []\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['dmn'])[0])\n",
    "# rois_notSMN = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "# rois_testing = np.asarray(rois_testing)\n",
    "# tmp = np.where(rois_testing>180)[0]\n",
    "# rois_testing = np.delete(rois_testing,tmp)\n",
    "\n",
    "h0, qs = mc.fdrcorrection0(statistics_rh[rois_testing,1])\n",
    "statistics_rh[:,1] = 1.0\n",
    "i = 0\n",
    "for roi in rois_testing:\n",
    "    statistics_rh[roi,1] = qs[i]\n",
    "    statistics_rh[roi,2] = h0[i]*statistics_rh[roi,0]\n",
    "    i += 1\n",
    "        \n",
    "nSignificant = np.sum(statistics_rh[:,1] < 0.05)\n",
    "print 'Number of significant transfers:', nSignificant\n",
    "\n",
    "if nSignificant>0:\n",
    "    sig_ind = np.where(statistics_rh[:,1]<0.05)[0]\n",
    "    for ind in sig_ind:\n",
    "        print '\\tSignificant parcel:', ind+1\n",
    "        print '\\tAccuracy:', statistics_rh[ind,0]\n",
    "        print '\\tNetwork:', networkdef[ind]\n",
    "print 'Network mappings:', networkmappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49479167  0.98853955  0.        ]\n",
      "[ 0.546875    0.36346843  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print statistics_rh[296]\n",
    "print statistics_rh[147]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "del actflow_data_rh, data_task_rh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run across subject decoding on left-hand motor responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_lh = h5py.File(outdir + 'ActFlowData_Target' + str(roi_lh) + '.h5','r')\n",
    "actflow_data_lh = h5f_lh['data'][:].copy()\n",
    "h5f_lh.close()\n",
    "\n",
    "data_task_lh = np.zeros((len(glasser2),nResponses,len(subjNums)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task_lh[:,:,scount] = loadMotorResponses(subj, hand='Left')\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding roi 13\n",
      "Decoding roi 14\n",
      "Decoding roi 28\n",
      "Decoding roi 62\n",
      "Decoding roi 72\n",
      "Decoding roi 76\n",
      "Decoding roi 79\n",
      "Decoding roi 81\n",
      "Decoding roi 82\n",
      "Decoding roi 84\n",
      "Decoding roi 88\n",
      "Decoding roi 90\n",
      "Decoding roi 91\n",
      "Decoding roi 96\n",
      "Decoding roi 97\n",
      "Decoding roi 110\n",
      "Decoding roi 132\n",
      "Decoding roi 143\n",
      "Decoding roi 144\n",
      "Decoding roi 148\n",
      "Decoding roi 169\n",
      "Decoding roi 170\n",
      "Decoding roi 193\n",
      "Decoding roi 194\n",
      "Decoding roi 208\n",
      "Decoding roi 237\n",
      "Decoding roi 241\n",
      "Decoding roi 242\n",
      "Decoding roi 252\n",
      "Decoding roi 253\n",
      "Decoding roi 256\n",
      "Decoding roi 259\n",
      "Decoding roi 260\n",
      "Decoding roi 262\n",
      "Decoding roi 264\n",
      "Decoding roi 268\n",
      "Decoding roi 270\n",
      "Decoding roi 271\n",
      "Decoding roi 272\n",
      "Decoding roi 276\n",
      "Decoding roi 277\n",
      "Decoding roi 290\n",
      "Decoding roi 312\n",
      "Decoding roi 323\n",
      "Decoding roi 324\n",
      "Decoding roi 328\n",
      "Decoding roi 341\n",
      "Decoding roi 349\n",
      "Decoding roi 350\n",
      "Decoding roi 356\n",
      "Decoding roi 10\n",
      "Decoding roi 45\n",
      "Decoding roi 49\n",
      "Decoding roi 94\n",
      "Decoding roi 95\n",
      "Decoding roi 115\n",
      "Decoding roi 116\n",
      "Decoding roi 126\n",
      "Decoding roi 135\n",
      "Decoding roi 136\n",
      "Decoding roi 142\n",
      "Decoding roi 145\n",
      "Decoding roi 225\n",
      "Decoding roi 229\n",
      "Decoding roi 274\n",
      "Decoding roi 275\n",
      "Decoding roi 295\n",
      "Decoding roi 296\n",
      "Decoding roi 306\n",
      "Decoding roi 315\n",
      "Decoding roi 316\n",
      "Decoding roi 322\n",
      "Decoding roi 325\n",
      "Decoding roi 9\n",
      "Decoding roi 36\n",
      "Decoding roi 37\n",
      "Decoding roi 42\n",
      "Decoding roi 43\n",
      "Decoding roi 44\n",
      "Decoding roi 56\n",
      "Decoding roi 57\n",
      "Decoding roi 58\n",
      "Decoding roi 59\n",
      "Decoding roi 77\n",
      "Decoding roi 83\n",
      "Decoding roi 85\n",
      "Decoding roi 98\n",
      "Decoding roi 104\n",
      "Decoding roi 105\n",
      "Decoding roi 107\n",
      "Decoding roi 108\n",
      "Decoding roi 112\n",
      "Decoding roi 113\n",
      "Decoding roi 146\n",
      "Decoding roi 147\n",
      "Decoding roi 166\n",
      "Decoding roi 168\n",
      "Decoding roi 177\n",
      "Decoding roi 178\n",
      "Decoding roi 179\n",
      "Decoding roi 189\n",
      "Decoding roi 190\n",
      "Decoding roi 204\n",
      "Decoding roi 216\n",
      "Decoding roi 217\n",
      "Decoding roi 222\n",
      "Decoding roi 223\n",
      "Decoding roi 224\n",
      "Decoding roi 236\n",
      "Decoding roi 238\n",
      "Decoding roi 239\n",
      "Decoding roi 257\n",
      "Decoding roi 261\n",
      "Decoding roi 263\n",
      "Decoding roi 265\n",
      "Decoding roi 278\n",
      "Decoding roi 284\n",
      "Decoding roi 285\n",
      "Decoding roi 287\n",
      "Decoding roi 288\n",
      "Decoding roi 292\n",
      "Decoding roi 293\n",
      "Decoding roi 326\n",
      "Decoding roi 327\n",
      "Decoding roi 346\n",
      "Decoding roi 348\n",
      "Decoding roi 357\n",
      "Decoding roi 358\n",
      "Decoding roi 359\n"
     ]
    }
   ],
   "source": [
    "nproc = 20\n",
    "ncvs = 1\n",
    "\n",
    "rois = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "rois = [189] # Right S1\n",
    "\n",
    "sourceROIs = []\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "\n",
    "distances_baseline_lh = np.zeros((nParcels,len(subjNums)*2))\n",
    "for roi in sourceROIs:\n",
    "    print 'Decoding roi', roi\n",
    "#     if roi in bad_rois: \n",
    "#         continue\n",
    "\n",
    "    distances_baseline_lh[roi,:] = motorResponseDecodings(data_task_lh, \n",
    "                                                          actflow_data_lh[:,:,:,roi],\n",
    "                                                          rois=rois, ncvs=ncvs, nproc=nproc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant transfers: 7\n",
      "\tSignificant parcel: 80\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 97\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 148\n",
      "\tAccuracy: 0.609375\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 194\n",
      "\tAccuracy: 0.609375\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 218\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 264\n",
      "\tAccuracy: 0.619791666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 276\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 5.0\n",
      "Network mappings: {'vis2': 2, 'lan': 6, 'vis1': 1, 'none2': 12, 'none1': 11, 'dan': 5, 'aud': 8, 'pmulti': 10, 'fpn': 7, 'dmn': 9, 'smn': 3, 'con': 4}\n"
     ]
    }
   ],
   "source": [
    "statistics_lh = np.zeros((distances_baseline_lh.shape[0],3))\n",
    "for roicount in range(distances_baseline_lh.shape[0]):\n",
    "    ntrials = len(subjNums)*2\n",
    "    p = stats.binom_test(np.mean(distances_baseline_lh[roicount,:])*ntrials,n=ntrials,p=0.5)\n",
    "    if np.mean(distances_baseline_lh[roicount,:])>0.5:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0-p/2.0\n",
    "\n",
    "    statistics_lh[roicount,0] = np.mean(distances_baseline_lh[roicount,:])\n",
    "    statistics_lh[roicount,1] = p\n",
    "\n",
    "rois_testing = []\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "rois_testing.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "# rois_notSMN = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "h0, qs = mc.fdrcorrection0(statistics_lh[rois_testing,1])\n",
    "statistics_lh[:,1] = 1.0\n",
    "i = 0\n",
    "for roi in rois_testing:\n",
    "    statistics_lh[roi,1] = qs[i]\n",
    "    statistics_lh[roi,2] = h0[i]*statistics_lh[roi,0]\n",
    "    i += 1\n",
    "        \n",
    "nSignificant = np.sum(statistics_lh[:,1] < 0.05)\n",
    "print 'Number of significant transfers:', nSignificant\n",
    "\n",
    "if nSignificant>0:\n",
    "    sig_ind = np.where(statistics_lh[:,1]<0.05)[0]\n",
    "    for ind in sig_ind:\n",
    "        print '\\tSignificant parcel:', ind+1\n",
    "        print '\\tAccuracy:', statistics_lh[ind,0]\n",
    "        print '\\tNetwork:', networkdef[ind]\n",
    "print 'Network mappings:', networkmappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46354167  0.99967248  0.        ]\n",
      "[ 0.609375    0.03211324  0.609375  ]\n"
     ]
    }
   ],
   "source": [
    "print statistics_lh[296]\n",
    "print statistics_lh[147]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "del actflow_data_lh, data_task_lh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
