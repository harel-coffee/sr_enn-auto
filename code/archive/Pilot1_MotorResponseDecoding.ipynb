{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot 1 -- Compute the baseline decodability of Motor rule response (LINDEX v. LMID and RINDEX v. RMID)\n",
    "\n",
    "## Use SVM classifications to decode hand-specific responses\n",
    "\n",
    "## Takuya Ito\n",
    "#### 02/23/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as stats\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import statsmodels.api as sm\n",
    "import sklearn.svm as svm\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import sklearn\n",
    "from sklearn.feature_selection import f_classif\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"FreeSans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 084\n",
    "subjNums = ['013','014','016','017','018','021','023','024','026','027','028','030','031','032','033',\n",
    "            '034','035','037','038','039','040','041','042','043','045','046','047','048','049','050',\n",
    "            '053','055','056','057','058','062','063','066','067','068','069','070','072','074','075',\n",
    "            '076','077','081','085','086','087','088','090','092','093','094','095','097','098','099',\n",
    "            '101','102','103','104','105','106','108','109','110','111','112','114','115','117','119',\n",
    "            '120','121','122','123','124','125','126','127','128','129','130','131','132','134','135',\n",
    "            '136','137','138','139','140','141']\n",
    "\n",
    "\n",
    "\n",
    "basedir = '/projects3/SRActFlow/'\n",
    "\n",
    "# Using final partition\n",
    "networkdef = np.loadtxt('/projects3/NetworkDiversity/data/network_partition.txt')\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "# network mappings for final partition set\n",
    "networkmappings = {'fpn':7, 'vis1':1, 'vis2':2, 'smn':3, 'aud':8, 'lan':6, 'dan':5, 'con':4, 'dmn':9, \n",
    "                   'pmulti':10, 'none1':11, 'none2':12}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "\n",
    "## General parameters/variables\n",
    "nParcels = 360\n",
    "nSubjs = len(subjNums)\n",
    "\n",
    "glasserfile2 = '/projects/AnalysisTools/ParcelsGlasser2016/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_RL.dlabel.nii'\n",
    "glasser2 = nib.load(glasserfile2).get_data()\n",
    "glasser2 = np.squeeze(glasser2)\n",
    "\n",
    "sortednets = np.sort(xticks.keys())\n",
    "orderednetworks = []\n",
    "for net in sortednets: orderednetworks.append(xticks[net])\n",
    "    \n",
    "networkpalette = ['royalblue','slateblue','paleturquoise','darkorchid','limegreen',\n",
    "                  'lightseagreen','yellow','orchid','r','peru','orange','olivedrab']\n",
    "networkpalette = np.asarray(networkpalette)\n",
    "\n",
    "OrderedNetworks = ['VIS1','VIS2','SMN','CON','DAN','LAN','FPN','AUD','DMN','PMM','VMM','ORA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Define functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMotorResponses(subj):\n",
    "    datadir = basedir + 'data/results/GLM_MotorResponse/' \n",
    "    data = np.loadtxt(datadir + subj + '_motorResponse_taskBetas_Surface64k_GSR.csv',delimiter=',')\n",
    "    data = data[:,-4:]\n",
    "    return data\n",
    "\n",
    "# def loadRestFC(subj,fctype='pearson',gsr=True):\n",
    "#     datadir = '/projects3/NetworkDiversity/data/hcppreprocessed/' + subj + '/' \n",
    "#     if fctype=='pearson':\n",
    "#         if gsr:\n",
    "#             data = np.loadtxt(datadir + subj + '_rest1_nuisanceResids_Glasser_GSR.csv',delimiter=',')\n",
    "#         else:\n",
    "#             data = np.loadtxt(datadir + subj + '_rest1_nuisanceResids_Glasser.csv',delimiter=',')\n",
    "#         fc = np.corrcoef(data)\n",
    "#     elif fctype=='multregfc':\n",
    "#         if gsr:\n",
    "#             direc = '/projects3/NetworkDiversity/data/results/multRegRestFC_GlasserParcels_HCPData/' \n",
    "#             fc = np.loadtxt(direc + subj + '_multreg_restfc_gsr.txt', delimiter=',')\n",
    "#         else:\n",
    "#             direc = '/projects3/NetworkDiversity/data/results/multRegRestFC_GlasserParcels_HCPData/' \n",
    "#             fc = np.loadtxt(direc + subj + '_multreg_restfc.txt', delimiter=',')\n",
    "# #             fc = np.loadtxt(direc + subj + '_OutNetToFPN_multreg_restfc.txt', delimiter=',')\n",
    "#     return fc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nResponses = 4\n",
    "data_task = np.zeros((len(glasser2),nResponses,len(subjNums)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task[:,:,scount] = loadMotorResponses(subj)\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define functions for motor response decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def motorResponseDecodings(data, ncvs=100, nproc=5):\n",
    "    \"\"\"\n",
    "    Run an across-subject classification\n",
    "    Decode responses on each hand separately from CPRO data\n",
    "    Limit to ROIs within SMN network\n",
    "    \"\"\"\n",
    "    \n",
    "    hands = {'Left':[0,1],'Right':[2,3]}\n",
    "    \n",
    "    nSubjs = data.shape[2]\n",
    "    nHands = len(hands)\n",
    "    rois = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "    smnStats = np.zeros((len(rois),nSubjs,nHands))\n",
    "\n",
    "    taskcount = 0\n",
    "    for hand in hands:\n",
    "        print 'Computing SVM classification for task', hand\n",
    "\n",
    "        nfing = len(hands[hand])\n",
    "        hand_ind = hands[hand]\n",
    "        \n",
    "        nsamples = nSubjs * nfing\n",
    "\n",
    "        # Label array for supervised learning\n",
    "        labels = np.tile(range(nfing),nSubjs)\n",
    "        subjarray = np.repeat(range(nSubjs),nfing)\n",
    "\n",
    "        # Run SVM classifications on network-level activation patterns across subjects\n",
    "        roicount = 0\n",
    "        for roi in rois:\n",
    "            roi_ind = np.where(glasser2==roi)[0]\n",
    "            nfeatures = len(roi_ind)\n",
    "            roi_ind.shape = (len(roi_ind),1)       \n",
    "\n",
    "            svm_mat = np.zeros((nsamples,roi_ind.shape[0]))\n",
    "            samplecount = 0\n",
    "            scount = 0\n",
    "            for subj in range(len(subjNums)):\n",
    "                roidata = np.squeeze(data[roi_ind,hand_ind,scount])\n",
    "                svm_mat[samplecount:(samplecount+nfing),:] = roidata.T\n",
    "\n",
    "                scount += 1\n",
    "                samplecount += nfing\n",
    "\n",
    "#             # Spatially demean matrix across features\n",
    "#             samplemean = np.mean(svm_mat,axis=1)\n",
    "#             samplemean.shape = (len(samplemean),1)\n",
    "#             svm_mat = svm_mat - samplemean\n",
    "            \n",
    "            scores = randomSplitLOOBaselineCV(ncvs, svm_mat, labels, subjarray, nproc=nproc)\n",
    "            smnStats[roicount,:,taskcount] = scores\n",
    "            roicount += 1\n",
    "        \n",
    "        taskcount += 1\n",
    "\n",
    "    return smnStats\n",
    "\n",
    "def randomSplitLOOBaselineCV(ncvs, svm_mat, labels, subjarray, nproc=5):\n",
    "    \"\"\"\n",
    "    Runs cross validation for an across-subject SVM analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    ntasks = len(np.unique(labels))\n",
    "    nsamples = svm_mat.shape[0]\n",
    "    nsubjs = nsamples/ntasks\n",
    "\n",
    "    subjects = np.unique(subjarray)\n",
    "    indices = np.arange(nsamples)\n",
    "\n",
    "    inputs = [] \n",
    "    for subj in subjects:\n",
    "        test_subj = subj\n",
    "        train_subjs_all = np.delete(subjects,test_subj)\n",
    "        for cv in range(ncvs):\n",
    "            # Randomly sample half of train set subjects for each cv (CV bootstrapping)\n",
    "            test_subjs = np.random.choice(train_subjs_all,\n",
    "                                         int(np.floor(len(train_subjs_all)/2.0)),\n",
    "                                         replace=True)\n",
    "            # Include all subjs not in train subjs into test subjs for mahalanobis computation\n",
    "            test_subjs = np.hstack((test_subj,test_subjs))\n",
    "            train_subjs = np.delete(subjects,test_subjs)\n",
    "\n",
    "            train_ind = []\n",
    "            for subj in train_subjs:\n",
    "                train_ind.extend(np.where(subjarray==subj)[0])\n",
    "\n",
    "            test_ind = []\n",
    "            for subj in test_subjs:\n",
    "                if subj == test_subj:\n",
    "                    # Keep track of how many samples this subject has\n",
    "                    subj_samples = len(np.where(subjarray==subj)[0]) \n",
    "                test_ind.extend(np.where(subjarray==subj)[0])\n",
    "            \n",
    "            train_ind = np.asarray(train_ind)\n",
    "            test_ind = np.asarray(test_ind)\n",
    "\n",
    "            trainset = svm_mat[train_ind,:]\n",
    "            testset = svm_mat[test_ind,:]\n",
    "\n",
    "            # Normalize trainset and testset\n",
    "            mean = np.mean(svm_mat[train_ind,:],axis=0)\n",
    "            mean.shape = (1,len(mean))\n",
    "            std = np.std(svm_mat[train_ind,:],axis=0)\n",
    "            std.shape = (1,len(std))\n",
    "\n",
    "            trainset = np.divide((trainset - mean),std)\n",
    "            testset = np.divide((testset - mean),std)\n",
    "\n",
    "            inputs.append((trainset,testset,labels[train_ind],labels[test_ind],subj_samples))\n",
    "        \n",
    "    pool = mp.Pool(processes=nproc)\n",
    "    scores = pool.map_async(_decoding,inputs).get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    subj_acc = np.zeros((len(subjects),))\n",
    "    scount = 0\n",
    "    i = 0\n",
    "    for subj in subjects:\n",
    "        subjmean = []\n",
    "        for cv in range(ncvs):\n",
    "            subjmean.append(scores[i])\n",
    "            i += 1\n",
    "        \n",
    "        subj_acc[scount] = np.mean(subjmean)\n",
    "        \n",
    "        scount += 1\n",
    "\n",
    "    return subj_acc\n",
    "\n",
    "def _decoding((trainset,testset,trainlabels,testlabels,subj_samples)):\n",
    "\n",
    "#     clf = sklearn.linear_model.LogisticRegression()\n",
    "    clf = svm.SVC(C=1.0, kernel='linear')\n",
    "\n",
    "    clf.fit(trainset,trainlabels)\n",
    "    predictions = clf.predict(testset)\n",
    "    acc = predictions==testlabels\n",
    "    score = np.mean(acc[0:subj_samples])\n",
    "    \n",
    "    return score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run across subject decoding on hand-specific motor responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVM classification for task Right\n",
      "Computing SVM classification for task Left\n"
     ]
    }
   ],
   "source": [
    "nproc = 25\n",
    "ncvs = 50\n",
    "\n",
    "distances_baseline = motorResponseDecodings(data_task, ncvs=ncvs, nproc=nproc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = {'Left':[0,1],'Right':[2,3]}\n",
    "smnROIs = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "\n",
    "statistics = {}\n",
    "taskcount = 0\n",
    "for hand in hands:\n",
    "    statistics[hand] = np.zeros((len(smnROIs),4)) # acc, t, p, q\n",
    "    for roicount in range(len(smnROIs)):\n",
    "        t, p = stats.ttest_1samp(distances_baseline[roicount,:,taskcount],1/float(len(hands[hand])))\n",
    "        if t > 0:\n",
    "            p = p/2.0\n",
    "        else:\n",
    "            p = 1.0 - p/2.0\n",
    "        \n",
    "        statistics[hand][roicount,0] = np.mean(distances_baseline[roicount,:,taskcount])\n",
    "        statistics[hand][roicount,1] = t\n",
    "        statistics[hand][roicount,2] = p\n",
    "        \n",
    "    qs = mc.fdrcorrection0(statistics[hand][:,2])[1]\n",
    "    for roicount in range(len(smnROIs)):\n",
    "        statistics[hand][roicount,3] = qs[roicount]\n",
    "    \n",
    "    taskcount += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ROIs significant for left hand responses: 5\n",
      "Accuracies: [ 0.57333333  0.69447917  0.5546875   0.55416667  0.55020833]\n",
      "\n",
      "Number of ROIs significant for right hand responses: 7\n",
      "Accuracies: [ 0.57729167  0.67        0.58322917  0.580625    0.54322917  0.53677083\n",
      "  0.57333333]\n"
     ]
    }
   ],
   "source": [
    "# Count number of significant ROIs for LH decoding\n",
    "sigLH_ind = np.where(statistics['Left'][:,3]<0.05)[0]\n",
    "print 'Number of ROIs significant for left hand responses:', sigLH_ind.shape[0]\n",
    "print 'Accuracies:', statistics['Left'][sigLH_ind,0]\n",
    "\n",
    "print ''\n",
    "# Count number of significant ROIs for RH decoding\n",
    "sigRH_ind = np.where(statistics['Right'][:,3]<0.05)[0]\n",
    "print 'Number of ROIs significant for right hand responses:', sigRH_ind.shape[0]\n",
    "print 'Accuracies:', statistics['Right'][sigRH_ind,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Map accuracies back to cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put all data into a single matrix (since we only run a single classification)\n",
    "lefthand = np.zeros((glasser2.shape[0],4))\n",
    "righthand = np.zeros((glasser2.shape[0],4))\n",
    "\n",
    "lh_ind = [0,1]\n",
    "rh_ind = [0,1]\n",
    "roicount = 0\n",
    "for roi in smnROIs:\n",
    "    vertex_ind = np.where(glasser2==roi)[0]\n",
    "    lefthand[vertex_ind,0] = statistics['Left'][roicount,0]\n",
    "    lefthand[vertex_ind,1] = statistics['Left'][roicount,1]\n",
    "    lefthand[vertex_ind,2] = statistics['Left'][roicount,2]\n",
    "    if statistics['Left'][roicount,3] < 0.05: lefthand[vertex_ind,3] = statistics['Left'][roicount,0]\n",
    "\n",
    "    righthand[vertex_ind,0] = statistics['Right'][roicount,0]\n",
    "    righthand[vertex_ind,1] = statistics['Right'][roicount,1]\n",
    "    righthand[vertex_ind,2] = statistics['Right'][roicount,2]\n",
    "    if statistics['Right'][roicount,3] < 0.05: righthand[vertex_ind,3] = statistics['Right'][roicount,0]\n",
    "\n",
    "    roicount += 1\n",
    "\n",
    "    \n",
    "#### \n",
    "# Write file to csv and run wb_command\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_MotorResponse/'\n",
    "filename = 'smnDecodingsLH'\n",
    "np.savetxt(outdir + filename + '.csv', lefthand,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "outdir = '/projects3/SRActFlow/data/results/Decoding_MotorResponse/'\n",
    "filename = 'smnDecodingsRH'\n",
    "np.savetxt(outdir + filename + '.csv', righthand,fmt='%s')\n",
    "wb_file = filename + '.dscalar.nii'\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + wb_file + ' -reset-scalars'\n",
    "os.system(wb_command)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
