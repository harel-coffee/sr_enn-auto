{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StroopActFlow\n",
    "### Identify stimuli localizers for input layer for Stroop Model\n",
    "### Identify top-down task rule inputs (e.g., PFC) for Stroop Model\n",
    "### First use MVPA to identify ROIs for each stimulus type; then use GLM to find activation patterns\n",
    "### Using new GLM with rule and trial beta series model\n",
    "\n",
    "#### Taku Ito\n",
    "#### 02/22/17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import os\n",
    "import nibabel as nib\n",
    "from sklearn.svm import SVC\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertCSVToCIFTI64k(inputfilename,outputfilename):\n",
    "    ciftitemplate = '/projects3/StroopActFlow/data/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "    wb_command = 'wb_command -cifti-convert -from-text' \n",
    "    wb_command += ' ' + inputfilename \n",
    "    wb_command += ' ' + ciftitemplate\n",
    "    wb_command += ' ' + outputfilename\n",
    "    wb_command += \" -col-delim ','\"\n",
    "    wb_command += ' -reset-scalars'\n",
    "    os.system(wb_command)\n",
    "#     print wb_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjNums = ['101', '102']\n",
    "\n",
    "basedir = '/projects3/StroopActFlow/data/'\n",
    "resultsdir = basedir + 'results/'\n",
    "restdir = resultsdir + 'glmRest_GlasserParcels/'\n",
    "\n",
    "glasser_nets = np.loadtxt('/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/community_order.txt', delimiter=',')\n",
    "# Make into python numbering (starts from 0)\n",
    "glasser_nets -= 1.0\n",
    "networkorder = glasser_nets.astype(int)\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "\n",
    "networkmappings = {'fpn':7, 'vis':1, 'smn':2, 'con':3, 'dmn':6, 'aud1':8, 'aud2':9, 'dan':11}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "networkdef = '/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/parcel_network_assignments.txt'\n",
    "networkdef = np.loadtxt(networkdef, delimiter=',')\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "    \n",
    "# Load in Glasser parcels\n",
    "glasserfile = '/projects3/StroopActFlow/data/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "glasser = nib.load(glasserfile).get_data()\n",
    "glasser = np.squeeze(glasser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Run task activation analysis on 64k Surface for four different stimuli localizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadStimulusBetas(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_ruleStimBetaSeries/'\n",
    "    betas = np.loadtxt(datadir + subj + '_RuleAndStimBetaSeries_taskBetas_Surface64k.csv',delimiter=',')\n",
    "    \n",
    "    nruleBetas = 60\n",
    "    \n",
    "    betas = betas[:,18+nruleBetas:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Run 2 MVPA analysis \n",
    "### Color Localizer: Green V Red\n",
    "### Word Localizer: Green V Red\n",
    "* Stim 1: Neutral Color Green\n",
    "* Stim 2: Neutral Color Red\n",
    "* Stim 3: Neutral Word Green\n",
    "* Stim 4: Neutral Word Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load in data as dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading betas for subject 101\n",
      "Loading betas for subject 102\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 45\n",
    "nstims = 4\n",
    "colorstims = [0,1] # First 2 are color\n",
    "wordstims = [2,3] # second 2 stims are word\n",
    "\n",
    "beta_dict = {}\n",
    "behavdata = {}\n",
    "\n",
    "# Organize subject betas by stimulus\n",
    "for subj in subjNums:\n",
    "    print 'Loading betas for subject', subj\n",
    "    betas = loadStimulusBetas(subj)\n",
    "    behavdata[subj] = utils.loadBehavData(subj)\n",
    "    # Get indices for neutral stims\n",
    "    neutral_ind = np.where(behavdata[subj]['condition']=='neutral')[0]\n",
    "    # Get color stims\n",
    "    colorgreen = np.where(behavdata[subj]['colorStim']=='green')[0]\n",
    "    neutral_colorgreen_ind = np.intersect1d(neutral_ind,colorgreen)\n",
    "    colorred = np.where(behavdata[subj]['colorStim']=='red')[0]\n",
    "    neutral_colorred_ind = np.intersect1d(neutral_ind, colorred)    \n",
    "    # Get word stims\n",
    "    wordgreen = np.where(behavdata[subj]['wordStim']=='GREEN')[0]\n",
    "    neutral_wordgreen_ind = np.intersect1d(neutral_ind,wordgreen)\n",
    "    wordred = np.where(behavdata[subj]['wordStim']=='RED')[0]\n",
    "    neutral_wordred_ind = np.intersect1d(neutral_ind, wordred)\n",
    "    \n",
    "    # Organize beta dict\n",
    "    beta_dict[subj] = np.zeros((betas.shape[0],ntrials_perstim,nstims))\n",
    "    beta_dict[subj][:,:,0] = betas[:,neutral_colorgreen_ind]\n",
    "    beta_dict[subj][:,:,1] = betas[:,neutral_colorred_ind]\n",
    "    beta_dict[subj][:,:,2] = betas[:,neutral_wordgreen_ind]\n",
    "    beta_dict[subj][:,:,3] = betas[:,neutral_wordred_ind]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 2 classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runCV(svmmat, labels, cvs=10, leaveout=5, replacement=False):\n",
    "    \"\"\"\n",
    "    Runs a cross validation given an svm matrix, labels, number of cross validations, and \n",
    "    number of leave outs for test predictions\n",
    "    \n",
    "    Assumes cross validation without replacement (for each train v. test prediction)\n",
    "    \"\"\"\n",
    "    # Get number of classes\n",
    "    classes = np.unique(labels)\n",
    "    \n",
    "    # Run cross-validation using Linear SVMs\n",
    "    leaveout_total = leaveout*len(classes)\n",
    "\n",
    "    # Spatially demean (across features, to ensure that mean activity does not factor into classification)\n",
    "    spatialmean = np.mean(svmmat,axis=1)\n",
    "    spatialmean.shape = (len(spatialmean),1)\n",
    "    svmmat = svmmat - spatialmean\n",
    "    \n",
    "    # Start cross-validation\n",
    "    nsamples = svmmat.shape[0]\n",
    "    acc = []\n",
    "    for cv in range(cvs):\n",
    "        test_ind = []\n",
    "        for stim in classes:\n",
    "            stimlabs = np.where(labels==stim)[0]\n",
    "            test_ind.extend(np.random.choice(stimlabs,leaveout,replace=replacement))\n",
    "            \n",
    "        test_ind = np.asarray(test_ind)\n",
    "        train_ind = np.delete(np.arange(nsamples),test_ind)\n",
    "\n",
    "        trainset = svmmat[train_ind,:]\n",
    "        testset = svmmat[test_ind,:]\n",
    "        # normalize train and test set according to train set mean & std\n",
    "        mean = np.mean(trainset,axis=0)\n",
    "        std = np.std(trainset,axis=0)\n",
    "        trainset = np.divide((trainset - mean),std)\n",
    "        testset = np.divide((testset - mean),std)\n",
    "\n",
    "        # Construct classifier and fit\n",
    "        svc = SVC(kernel='linear')\n",
    "        svc.fit(trainset,labels[train_ind])\n",
    "        # Get acc\n",
    "        acc.append(svc.score(testset,labels[test_ind]))\n",
    "\n",
    "    # Get average accuracy\n",
    "#     accmat[subj][roi_ind,0] = np.mean(acc)\n",
    "    avg_acc = np.mean(acc)\n",
    "\n",
    "    \n",
    "    # Perform within-subject binomial test\n",
    "    ntotal = leaveout_total*cvs\n",
    "    nsuccess = avg_acc*ntotal\n",
    "    chance = 1.0/len(classes)\n",
    "    p = stats.binom_test(nsuccess,ntotal,p=chance)\n",
    "\n",
    "    # Make sure it's a one-sided binomial test\n",
    "    if avg_acc > chance:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0 - p/2.0\n",
    "    \n",
    "    return avg_acc, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run actual code/cross-validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running classifications for subject 101\n",
      "Running classifications for subject 102\n"
     ]
    }
   ],
   "source": [
    "accmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "accmat_thresh = {}\n",
    "nparcels = 360\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running classifications for subject', subj\n",
    "    accmat[subj] = np.zeros((glasser.shape[0],3)) # 2 for two classifications; 3rd is for 4-way class    \n",
    "    pmat[subj] = np.zeros((nparcels,3))\n",
    "    \n",
    "    for roi in range(nparcels):\n",
    "        if nparcels%100==0:\n",
    "            print 'Running classifications on ROI,', roi + 1\n",
    "        roi_ind = np.where(glasser==roi+1)[0]\n",
    "        \n",
    "        ####\n",
    "        ## Color STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        mat_tmp = [] # For a 2-way classification\n",
    "        labels_tmp = [] \n",
    "        for stim in colorstims:\n",
    "            mat_tmp.append(beta_dict[subj][roi_ind,:,stim].T)\n",
    "            labels_tmp.append(np.ones((ntrials_perstim,1))*stim)\n",
    "        svmmat_color = np.vstack(mat_tmp)\n",
    "        labels_color = np.squeeze(np.vstack(labels_tmp))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_color, labels_color, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,0] = acc\n",
    "        pmat[subj][roi,0] = p\n",
    "#         h0, qs = mc.fdrcorrection0(pmat[subj][:,0])\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## Word STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        mat_tmp = [] # For a 2-way classification\n",
    "        labels_tmp = [] \n",
    "        for stim in wordstims:\n",
    "            mat_tmp.append(beta_dict[subj][roi_ind,:,stim].T)\n",
    "            labels_tmp.append(np.ones((ntrials_perstim,1))*stim)\n",
    "        svmmat_word = np.vstack(mat_tmp)\n",
    "        labels_word = np.squeeze(np.vstack(labels_tmp))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_word, labels_word, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,1] = acc\n",
    "        pmat[subj][roi,1] = p\n",
    "        \n",
    "        \n",
    "        ####\n",
    "        ## 4-way STIM classification        \n",
    "        # Set up SVM Matrix\n",
    "        svmmat_4way = np.vstack((svmmat_color,svmmat_word))\n",
    "        labels_4way = np.squeeze(np.hstack((labels_color,labels_word)))\n",
    "        \n",
    "        # Run cross-validation using Linear SVMs\n",
    "        crossvalidations = 10 # 10-fold cross validation; 90 samples, 81 train; 9 test\n",
    "        leaveout = 5 # leave out 5 samples of each stim\n",
    "\n",
    "        acc, p = runCV(svmmat_4way, labels_4way, cvs=crossvalidations, leaveout=leaveout, replacement=False)\n",
    "        accmat[subj][roi_ind,2] = acc\n",
    "        pmat[subj][roi,2] = p\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run FDR correction for each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qmat = {}\n",
    "mvpa_mask = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    mvpa_mask[subj] = np.zeros(accmat[subj].shape)\n",
    "    \n",
    "    for test in range(accmat[subj].shape[1]):\n",
    "        h0,qs = mc.fdrcorrection0(pmat[subj][:,test])\n",
    "        \n",
    "        # Iterate through each ROI and make sure it is significant\n",
    "#         for roi in range(nparcels):\n",
    "#             roi_ind = np.where(glasser==roi+1)[0]\n",
    "#             if qs[roi] < 0.01:\n",
    "#                 mvpa_mask[subj][roi_ind,test] = 1.0\n",
    "        tmp = np.max(accmat[subj][:,test])\n",
    "        roi_ind = accmat[subj][:,test] == tmp\n",
    "#         for roi in rois: \n",
    "#             roi_ind = np.where(glasser==roi+1)[0]\n",
    "        mvpa_mask[subj][roi_ind,test] = 1.0\n",
    "        mvpa_mask[subj][:,test] = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Run GLM analysis on MVPA masks for Red V Green for Color and Word neutral stims separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colorstims = [0, 1]\n",
    "wordstims = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Perform univariate t-test on color stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttest_loc = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    ttest_loc[subj] = {}\n",
    "    # 1st column is for t-vals; 2nd for p; 3rd for FDR-corrected t-values\n",
    "    # Vertex X stats matrix\n",
    "    ttest_loc[subj]['color'] = np.zeros((mvpa_mask[subj].shape[0],3)) \n",
    "    ttest_loc[subj]['word'] = np.zeros((mvpa_mask[subj].shape[0],3)) \n",
    "\n",
    "    # significant vertices in MVPA mask\n",
    "    color_ind = np.where(mvpa_mask[subj][:,0])[0]\n",
    "    word_ind = np.where(mvpa_mask[subj][:,1])[0]\n",
    "    \n",
    "    # Perform color t-test first\n",
    "    for v in color_ind:\n",
    "        t, p = stats.ttest_rel(beta_dict[subj][v,:,colorstims[0]],\n",
    "                               beta_dict[subj][v,:,colorstims[1]])\n",
    "        ttest_loc[subj]['color'][v,0], ttest_loc[subj]['color'][v,1] = t, p\n",
    "    # Perform FDR-correction\n",
    "#     h0, q = mc.fdrcorrection0(ttest_loc[subj]['color'][color_ind,1])\n",
    "    h0 = ttest_loc[subj]['color'][color_ind,1] < 0.05\n",
    "    ttest_loc[subj]['color'][color_ind,2] = np.multiply(h0,ttest_loc[subj]['color'][color_ind,0])\n",
    "        \n",
    "    # Perform word t-test second\n",
    "    for v in word_ind:\n",
    "        t, p = stats.ttest_rel(beta_dict[subj][v,:,wordstims[0]],\n",
    "                               beta_dict[subj][v,:,wordstims[1]])\n",
    "        ttest_loc[subj]['word'][v,0], ttest_loc[subj]['word'][v,1] = t, p\n",
    "    # Perform FDR-correction\n",
    "#     h0, q = mc.fdrcorrection0(ttest_loc[subj]['word'][word_ind,1])\n",
    "    h0 = ttest_loc[subj]['word'][word_ind,1] < 0.05\n",
    "    ttest_loc[subj]['word'][word_ind,2] = np.multiply(h0,ttest_loc[subj]['word'][word_ind,0])\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Save statistical maps to CSV and dscalar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subj in subjNums:\n",
    "    basedir = '/projects3/StroopActFlow/data/results/glm_ruleStimBetaSeries/neutralStims_localizer/'\n",
    "    filecolor = basedir+subj+'_MVPAconstrained_ttestContrast_color_v2'\n",
    "    fileword = basedir+subj+'_MVPAconstrained_ttestContrast_word_v2'\n",
    "    \n",
    "    np.savetxt(filecolor+'.csv', ttest_loc[subj]['color'], delimiter=',')\n",
    "    np.savetxt(fileword+'.csv', ttest_loc[subj]['word'], delimiter=',')\n",
    "    \n",
    "    convertCSVToCIFTI64k(filecolor+'.csv', filecolor+'.dscalar.nii')\n",
    "    convertCSVToCIFTI64k(fileword+'.csv', fileword+'.dscalar.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qmat = {}\n",
    "mvpa_mask = {}\n",
    "accmat_thresh = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    mvpa_mask[subj] = np.zeros(accmat[subj].shape)\n",
    "    accmat_thresh[subj] = np.zeros(accmat[subj].shape)\n",
    "    for test in range(accmat[subj].shape[1]):\n",
    "        h0,qs = mc.fdrcorrection0(pmat[subj][:,test])\n",
    "        \n",
    "        # Iterate through each ROI and make sure it is significant\n",
    "        for roi in range(nparcels):\n",
    "            roi_ind = np.where(glasser==roi+1)[0]\n",
    "            if qs[roi] < 0.05:\n",
    "                mvpa_mask[subj][roi_ind,test] = 1.0\n",
    "            \n",
    "        accmat_thresh[subj][:,test] = np.multiply(mvpa_mask[subj][:,test],accmat[subj][:,test])\n",
    "        \n",
    "for subj in subjNums:\n",
    "    basedir = '/projects3/StroopActFlow/data/results/glm_ruleStimBetaSeries/neutralStims_localizer/'\n",
    "    filemvpaacc = basedir+subj+'_neutralStimuliMVPA_Acc_v2'\n",
    "    filemvpaaccthresh = basedir+subj+'_neutralStimuliMVPA_AccThresh_v2'\n",
    "    \n",
    "    np.savetxt(filemvpaacc+'.csv', accmat[subj], delimiter=',')\n",
    "    np.savetxt(filemvpaaccthresh+'.csv', accmat_thresh[subj], delimiter=',')\n",
    "    \n",
    "    convertCSVToCIFTI64k(filemvpaacc+'.csv', filemvpaacc+'.dscalar.nii')\n",
    "    convertCSVToCIFTI64k(filemvpaaccthresh+'.csv', filemvpaaccthresh+'.dscalar.nii')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
