{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI9a -- Compute the baseline decodability of Motor responses (LINDEX v. LMID and RINDEX v. RMID)\n",
    "## Using ActFlow, All to one, via PCA-FC\n",
    "\n",
    "## Use SVM classifications to decode hand-specific responses\n",
    "## Using Ciric-style postprocessing\n",
    "\n",
    "## Takuya Ito\n",
    "#### 02/24/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tito/miniconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import scipy.stats as stats\n",
    "import nibabel as nib\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(1)\n",
    "import statsmodels.api as sm\n",
    "import sklearn.svm as svm\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import sklearn\n",
    "from sklearn.feature_selection import f_classif\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "os.sys.path.append('glmScripts/')\n",
    "import taskGLMPipeline as tgp\n",
    "import statsmodels.api as sm\n",
    "import tools\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"FreeSans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 084\n",
    "subjNums = ['013','014','016','017','018','021','023','024','026','027','028','030','031','032','033',\n",
    "            '034','035','037','038','039','040','041','042','043','045','046','047','048','049','050',\n",
    "            '053','055','056','057','058','062','063','066','067','068','069','070','072','074','075',\n",
    "            '076','077','081','085','086','087','088','090','092','093','094','095','097','098','099',\n",
    "            '101','102','103','104','105','106','108','109','110','111','112','114','115','117','119',\n",
    "            '120','121','122','123','124','125','126','127','128','129','130','131','132','134','135',\n",
    "            '136','137','138','139','140','141']\n",
    "\n",
    "\n",
    "\n",
    "basedir = '/projects3/SRActFlow/'\n",
    "\n",
    "# Using final partition\n",
    "networkdef = np.loadtxt('/projects3/NetworkDiversity/data/network_partition.txt')\n",
    "networkorder = np.asarray(sorted(range(len(networkdef)), key=lambda k: networkdef[k]))\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "# network mappings for final partition set\n",
    "networkmappings = {'fpn':7, 'vis1':1, 'vis2':2, 'smn':3, 'aud':8, 'lan':6, 'dan':5, 'con':4, 'dmn':9, \n",
    "                   'pmulti':10, 'none1':11, 'none2':12}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net\n",
    "\n",
    "## General parameters/variables\n",
    "nParcels = 360\n",
    "nSubjs = len(subjNums)\n",
    "\n",
    "glasserfile2 = '/projects/AnalysisTools/ParcelsGlasser2016/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_RL.dlabel.nii'\n",
    "glasser2 = nib.load(glasserfile2).get_data()\n",
    "glasser2 = np.squeeze(glasser2)\n",
    "\n",
    "sortednets = np.sort(xticks.keys())\n",
    "orderednetworks = []\n",
    "for net in sortednets: orderednetworks.append(xticks[net])\n",
    "    \n",
    "networkpalette = ['royalblue','slateblue','paleturquoise','darkorchid','limegreen',\n",
    "                  'lightseagreen','yellow','orchid','r','peru','orange','olivedrab']\n",
    "networkpalette = np.asarray(networkpalette)\n",
    "\n",
    "OrderedNetworks = ['VIS1','VIS2','SMN','CON','DAN','LAN','FPN','AUD','DMN','PMM','VMM','ORA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Define functions for motor response decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def motorResponseDecodings(data, actflow_data, rois, ncvs=1, nproc=5):\n",
    "    \"\"\"\n",
    "    Run an across-subject classification\n",
    "    Decode responses on each hand separately from CPRO data\n",
    "    \"\"\"\n",
    "\n",
    "    nSubjs = data.shape[2]\n",
    "    stats = np.zeros((len(rois),))\n",
    "    \n",
    "    nfing = data.shape[1]\n",
    "\n",
    "    nsamples = nSubjs * nfing\n",
    "\n",
    "    # Label array for supervised learning\n",
    "    labels = np.tile(range(nfing),nSubjs)\n",
    "    subjarray = np.repeat(range(nSubjs),nfing)\n",
    "\n",
    "    # Run SVM classifications on network-level activation patterns across subjects\n",
    "    roicount = 0\n",
    "    for roi in rois:\n",
    "        roi_ind = np.where(glasser2==roi)[0]\n",
    "        nfeatures = len(roi_ind)\n",
    "        roi_ind.shape = (len(roi_ind),1)       \n",
    "\n",
    "        svm_mat = np.zeros((nsamples,roi_ind.shape[0]))\n",
    "        actflow_svm_mat = np.zeros((nsamples,roi_ind.shape[0]))\n",
    "        samplecount = 0\n",
    "        scount = 0\n",
    "        for subj in range(len(subjNums)):\n",
    "            roidata = np.squeeze(data[roi_ind,:,scount])\n",
    "            actflow_roidata = np.squeeze(actflow_data[roi_ind,:,scount])\n",
    "            svm_mat[samplecount:(samplecount+nfing),:] = roidata.T\n",
    "            actflow_svm_mat[samplecount:(samplecount+nfing),:] = actflow_roidata.T\n",
    "\n",
    "            scount += 1\n",
    "            samplecount += nfing\n",
    "\n",
    "            # Spatially demean matrix across features\n",
    "            samplemean = np.mean(svm_mat,axis=1)\n",
    "            samplemean.shape = (len(samplemean),1)\n",
    "            svm_mat = svm_mat - samplemean\n",
    "            \n",
    "            samplemean = np.mean(actflow_svm_mat,axis=1)\n",
    "            samplemean.shape = (len(samplemean),1)\n",
    "            actflow_svm_mat = actflow_svm_mat - samplemean\n",
    "\n",
    "        scores = randomSplitLOOBaselineCV(ncvs, svm_mat, actflow_svm_mat, labels, subjarray, nproc=nproc)\n",
    "        stats[roicount] = np.mean(scores)\n",
    "        roicount += 1\n",
    "        \n",
    "    return stats\n",
    "\n",
    "def randomSplitLOOBaselineCV(ncvs, svm_mat, actflow_svm_mat, labels, subjarray, nproc=5):\n",
    "    \"\"\"\n",
    "    Runs cross validation for an across-subject SVM analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    ntasks = len(np.unique(labels))\n",
    "    nsamples = svm_mat.shape[0]\n",
    "    nsubjs = nsamples/ntasks\n",
    "\n",
    "    subjects = np.unique(subjarray)\n",
    "    indices = np.arange(nsamples)\n",
    "    \n",
    "    numsubjs_perfold = 1\n",
    "    if nsubjs%numsubjs_perfold!=0: \n",
    "        raise Exception(\"Error: Folds don't match number of subjects\")\n",
    "        \n",
    "    nfolds = nsubjs/numsubjs_perfold\n",
    "    subj_array_folds = subjarray.copy()\n",
    "    \n",
    "    inputs = [] \n",
    "    \n",
    "    for fold in range(nfolds):\n",
    "        test_subjs = np.random.choice(subj_array_folds,numsubjs_perfold,replace=False)\n",
    "        train_subjs_all = np.delete(subjects,test_subjs)\n",
    "        for cv in range(ncvs):\n",
    "            # Randomly sample half of train set subjects for each cv (CV bootstrapping)\n",
    "            train_subjs = np.random.choice(train_subjs_all,\n",
    "                                         int(np.floor(len(train_subjs_all)*(4.0))),\n",
    "                                         replace=True)\n",
    "\n",
    "            train_ind = []\n",
    "            for subj in train_subjs:\n",
    "                train_ind.extend(np.where(subjarray==subj)[0])\n",
    "\n",
    "            test_ind = []\n",
    "            for subj in test_subjs:\n",
    "                test_ind.extend(np.where(subjarray==subj)[0])\n",
    "            \n",
    "            train_ind = np.asarray(train_ind)\n",
    "            test_ind = np.asarray(test_ind)\n",
    "\n",
    "            trainset = actflow_svm_mat[train_ind,:]\n",
    "            testset = svm_mat[test_ind,:]\n",
    "            orig_training = svm_mat[train_ind,:]\n",
    "\n",
    "            # Normalize trainset and testset\n",
    "            trainmean = np.mean(actflow_svm_mat[train_ind,:],axis=0)\n",
    "            trainmean.shape = (1,len(trainmean))\n",
    "            trainstd = np.std(actflow_svm_mat[train_ind,:],axis=0)\n",
    "            trainstd.shape = (1,len(trainstd))\n",
    "            \n",
    "            # Normalize trainset and testset\n",
    "            testmean = np.mean(svm_mat[train_ind,:],axis=0)\n",
    "            testmean.shape = (1,len(testmean))\n",
    "            teststd = np.std(svm_mat[train_ind,:],axis=0)\n",
    "            teststd.shape = (1,len(teststd))\n",
    "\n",
    "            trainset = np.divide((trainset - trainmean),trainstd)\n",
    "            testset = np.divide((testset - testmean),teststd)\n",
    "\n",
    "            ######## FEATURE SELECTION & REDUCTION\n",
    "            ## Feature selection and downsampling\n",
    "            trainlabels = labels[train_ind]\n",
    "            testlabels = labels[test_ind]\n",
    "            unique_labels = np.unique(labels)\n",
    "            feat1_labs = np.where(trainlabels==0)[0]\n",
    "            feat2_labs = np.where(trainlabels==1)[0]\n",
    "            # Perform t-test\n",
    "            t, p = stats.ttest_rel(orig_training[feat1_labs,:],orig_training[feat2_labs,:],axis=0)\n",
    "            h0, qs = mc.fdrcorrection0(p)\n",
    "            # Construct feature masks\n",
    "            feat1_mask = np.multiply(t<0,h0)\n",
    "            feat2_mask = np.multiply(t>0,h0)\n",
    "#             feat1_mask = t>0\n",
    "#             feat2_mask = t<0\n",
    "            # Downsample training set into original vertices into 2 ROI signals\n",
    "            trainset_downsampled = np.zeros((trainset.shape[0],2))\n",
    "            trainset_downsampled[:,0] = np.nanmean(trainset[:,feat1_mask],axis=1)\n",
    "            trainset_downsampled[:,1] = np.nanmean(trainset[:,feat2_mask],axis=1)\n",
    "            trainset_downsampled = trainset[:,h0]\n",
    "            # Downsample test set into original vertices\n",
    "            testset_downsampled = np.zeros((testset.shape[0],2))\n",
    "            testset_downsampled[:,0] = np.nanmean(testset[:,feat1_mask],axis=1)\n",
    "            testset_downsampled[:,1] = np.nanmean(testset[:,feat2_mask],axis=1)\n",
    "            testset_downsampled = testset[:,h0]\n",
    "#             print 'feat1_mask', np.sum(feat1_mask), '| feat2_mask', np.sum(feat2_mask)\n",
    "\n",
    "            if np.sum(feat1_mask)==0 or np.sum(feat2_mask)==0:\n",
    "                print 'not running feature selection'\n",
    "                inputs.append((trainset,testset,labels[train_ind],labels[test_ind]))\n",
    "            else:\n",
    "                inputs.append((trainset_downsampled,testset_downsampled,labels[train_ind],labels[test_ind]))\n",
    "\n",
    "#             inputs.append((trainset,testset,labels[train_ind],labels[test_ind]))         \n",
    "    \n",
    "        subj_array_folds = np.delete(subj_array_folds,test_subjs)\n",
    "        \n",
    "    pool = mp.Pool(processes=nproc)\n",
    "    scores = pool.map_async(_decoding,inputs).get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    acc = []\n",
    "    for score in scores:\n",
    "        acc.extend(score)\n",
    "    return acc\n",
    "\n",
    "def _decoding((trainset,testset,trainlabels,testlabels)):\n",
    "\n",
    "#     clf = sklearn.linear_model.LogisticRegression()\n",
    "#     clf = svm.SVC(C=1.0, kernel='linear')\n",
    "\n",
    "#     clf.fit(trainset,trainlabels)\n",
    "#     predictions = clf.predict(testset)\n",
    "#     acc = predictions==testlabels\n",
    "    \n",
    "    unique_cond = np.unique(trainlabels)\n",
    "    acc = []\n",
    "    for cond1 in unique_cond:\n",
    "        mismatches = []\n",
    "        prototype_ind = np.where(trainlabels==cond1)[0]\n",
    "        prototype = np.mean(trainset[prototype_ind,:],axis=0)\n",
    "        for cond2 in unique_cond:\n",
    "            test_ind = np.where(testlabels==cond2)[0]\n",
    "            test = np.mean(testset[test_ind,:],axis=0)\n",
    "            if cond1 == cond2: \n",
    "                correct = stats.spearmanr(prototype,test)[0]\n",
    "            else:\n",
    "                mismatches.append(stats.spearmanr(prototype,test)[0])\n",
    "#         print correct, mismatches\n",
    "        if correct > np.max(mismatches): \n",
    "            acc.append(1.0)\n",
    "        else:\n",
    "            acc.append(0.0)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Load data for RH responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = reload(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nResponses = 2\n",
    "data_task_rh = np.zeros((len(glasser2),nResponses,len(subjNums)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task_rh[:,:,scount] = tools.loadCrossTrialMotorResponses(subj,hand='Right')\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.02 Generate actflow data for RH responses (Left S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_rh = 9 #left S1\n",
    "roi_lh = 189\n",
    "target_rh_ind = np.where(glasser2==roi_rh)[0]\n",
    "\n",
    "dilateLH = tools.loadMask(roi_lh,dilated=True)\n",
    "dilateRH = tools.loadMask(roi_rh,dilated=True)\n",
    "combinedDilated = dilateLH + dilateRH\n",
    "# Exclude all SMN regions\n",
    "smn_rois = np.where(networkdef==networkmappings['smn'])[0]\n",
    "for x in smn_rois:\n",
    "    roi_ind = np.where(glasser2==x)[0]\n",
    "    combinedDilated[roi_ind]=1\n",
    "source_ind = np.where(combinedDilated==0)[0]\n",
    "\n",
    "fcmapping_rh = np.zeros((len(source_ind),len(target_rh_ind)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    fcmapping_rh[:,:] = fcmapping_rh[:,:] + tools.loadPcaFCNoColliders(subj,roi_rh)\n",
    "    scount += 1\n",
    "\n",
    "fcmapping_rh = np.divide(fcmapping_rh,len(subjNums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_rh = 9 # left s1\n",
    "roi_lh = 189 # right s1\n",
    "actflow_data = np.zeros((len(glasser2),nResponses,len(subjNums),nParcels))\n",
    "\n",
    "dilateLH = tools.loadMask(roi_lh,dilated=True)\n",
    "dilateRH = tools.loadMask(roi_rh,dilated=True)\n",
    "combinedDilated = dilateLH + dilateRH\n",
    "# Exclude all SMN regions\n",
    "smn_rois = np.where(networkdef==networkmappings['smn'])[0]\n",
    "for x in smn_rois:\n",
    "    roi_ind = np.where(glasser2==x)[0]\n",
    "    combinedDilated[roi_ind]=1\n",
    "source_space = np.where(combinedDilated==0)[0]\n",
    "\n",
    "fc_source_ind = glasser2[source_space].copy()\n",
    "\n",
    "bad_rois = []\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "#     print 'Subject', subj, '(', scount+1, '/', len(subjNums), ')'\n",
    "    for roi in range(nParcels):\n",
    "        source_ind = np.where(glasser2==roi+1)[0]\n",
    "        source_ind = np.intersect1d(source_ind,source_space) # Make sure no vertices are within 10mm of the target\n",
    "        if source_ind.shape[0]==0:\n",
    "#             print 'This source has no vertices, skipping'\n",
    "            bad_rois.append(roi)\n",
    "            continue\n",
    "        \n",
    "        fc_source_roi = np.where(fc_source_ind==roi+1)[0]\n",
    "        \n",
    "        target_ind = np.where(glasser2==roi_rh)[0]\n",
    "#         # Right Finger 1\n",
    "#         actflow_data[target_ind,0,scount,roi] = np.dot(stats.zscore(data_task_rh[source_ind,0,scount],axis=0),fcmapping_rh[fc_source_roi,:])\n",
    "#         # Right Finger 2\n",
    "#         actflow_data[target_ind,1,scount,roi] = np.dot(stats.zscore(data_task_rh[source_ind,1,scount],axis=0),fcmapping_rh[fc_source_roi,:])\n",
    "        # Right Finger 1\n",
    "        actflow_data[target_ind,0,scount,roi] = np.dot(data_task_rh[source_ind,0,scount],fcmapping_rh[fc_source_roi,:])\n",
    "        # Right Finger 2\n",
    "        actflow_data[target_ind,1,scount,roi] = np.dot(data_task_rh[source_ind,1,scount],fcmapping_rh[fc_source_roi,:])\n",
    "\n",
    "    scount += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run across subject decoding on right-hand motor responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding roi 13\n",
      "Decoding roi 14\n",
      "Decoding roi 28\n",
      "Decoding roi 62\n",
      "Decoding roi 72\n",
      "Decoding roi 76\n",
      "Decoding roi 79\n",
      "Decoding roi 81\n",
      "Decoding roi 82\n",
      "Decoding roi 84\n",
      "Decoding roi 88\n",
      "Decoding roi 90\n",
      "Decoding roi 91\n",
      "Decoding roi 96\n",
      "Decoding roi 97\n",
      "Decoding roi 110\n",
      "Decoding roi 132\n",
      "Decoding roi 143\n",
      "Decoding roi 144\n",
      "Decoding roi 148\n",
      "Decoding roi 169\n",
      "Decoding roi 170\n",
      "Decoding roi 193\n",
      "Decoding roi 194\n",
      "Decoding roi 208\n",
      "Decoding roi 237\n",
      "Decoding roi 241\n",
      "Decoding roi 242\n",
      "Decoding roi 252\n",
      "Decoding roi 253\n",
      "Decoding roi 256\n",
      "Decoding roi 259\n",
      "Decoding roi 260\n",
      "Decoding roi 262\n",
      "Decoding roi 264\n",
      "Decoding roi 268\n",
      "Decoding roi 270\n",
      "Decoding roi 271\n",
      "Decoding roi 272\n",
      "Decoding roi 276\n",
      "Decoding roi 277\n",
      "Decoding roi 290\n",
      "Decoding roi 312\n",
      "Decoding roi 323\n",
      "Decoding roi 324\n",
      "Decoding roi 328\n",
      "Decoding roi 341\n",
      "Decoding roi 349\n",
      "Decoding roi 350\n",
      "Decoding roi 356\n",
      "Decoding roi 10\n",
      "Decoding roi 45\n",
      "Decoding roi 49\n",
      "Decoding roi 94\n",
      "Decoding roi 95\n",
      "Decoding roi 115\n",
      "Decoding roi 116\n",
      "Decoding roi 126\n",
      "Decoding roi 135\n",
      "Decoding roi 136\n",
      "Decoding roi 142\n",
      "Decoding roi 145\n",
      "Decoding roi 225\n",
      "Decoding roi 229\n",
      "Decoding roi 274\n",
      "Decoding roi 275\n",
      "Decoding roi 295\n",
      "Decoding roi 296\n",
      "Decoding roi 306\n",
      "Decoding roi 315\n",
      "Decoding roi 316\n",
      "Decoding roi 322\n",
      "Decoding roi 325\n",
      "Decoding roi 9\n",
      "Decoding roi 36\n",
      "Decoding roi 37\n",
      "Decoding roi 42\n",
      "Decoding roi 43\n",
      "Decoding roi 44\n",
      "Decoding roi 56\n",
      "Decoding roi 57\n",
      "Decoding roi 58\n",
      "Decoding roi 59\n",
      "Decoding roi 77\n",
      "Decoding roi 83\n",
      "Decoding roi 85\n",
      "Decoding roi 98\n",
      "Decoding roi 104\n",
      "Decoding roi 105\n",
      "Decoding roi 107\n",
      "Decoding roi 108\n",
      "Decoding roi 112\n",
      "Decoding roi 113\n",
      "Decoding roi 146\n",
      "Decoding roi 147\n",
      "Decoding roi 166\n",
      "Decoding roi 168\n",
      "Decoding roi 177\n",
      "Decoding roi 178\n",
      "Decoding roi 179\n",
      "Decoding roi 189\n",
      "Decoding roi 190\n",
      "Decoding roi 204\n",
      "Decoding roi 216\n",
      "Decoding roi 217\n",
      "Decoding roi 222\n",
      "Decoding roi 223\n",
      "Decoding roi 224\n",
      "Decoding roi 236\n",
      "Decoding roi 238\n",
      "Decoding roi 239\n",
      "Decoding roi 257\n",
      "Decoding roi 261\n",
      "Decoding roi 263\n",
      "Decoding roi 265\n",
      "Decoding roi 278\n",
      "Decoding roi 284\n",
      "Decoding roi 285\n",
      "Decoding roi 287\n",
      "Decoding roi 288\n",
      "Decoding roi 292\n",
      "Decoding roi 293\n",
      "Decoding roi 326\n",
      "Decoding roi 327\n",
      "Decoding roi 346\n",
      "Decoding roi 348\n",
      "Decoding roi 357\n",
      "Decoding roi 358\n",
      "Decoding roi 359\n"
     ]
    }
   ],
   "source": [
    "nproc = 20\n",
    "ncvs = 1\n",
    "\n",
    "rois = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "rois = [9] # Left S1\n",
    "\n",
    "\n",
    "sourceROIs = []\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "# sourceROIs = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "\n",
    "distances_baseline_rh = np.zeros((nParcels,len(subjNums)*2))\n",
    "for roi in sourceROIs:\n",
    "    print 'Decoding roi', roi\n",
    "    if roi in bad_rois: \n",
    "        continue\n",
    "\n",
    "    distances_baseline_rh[roi,:] = motorResponseDecodings(data_task_rh,\n",
    "                                                          actflow_data[:,:,:,roi],\n",
    "                                                          rois=rois, ncvs=ncvs, nproc=nproc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant transfers: 33\n",
      "Number of significant effects 33\n",
      "Left hemisphere effects: 19\n",
      "Right hemisphere effects: 14\n",
      "\tSignificant parcel: 10\n",
      "\tAccuracy: 0.6875\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 14\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 37\n",
      "\tAccuracy: 0.666666666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 43\n",
      "\tAccuracy: 0.770833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 44\n",
      "\tAccuracy: 0.75\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 59\n",
      "\tAccuracy: 0.822916666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 77\n",
      "\tAccuracy: 0.666666666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 78\n",
      "\tAccuracy: 0.65625\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 83\n",
      "\tAccuracy: 0.760416666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 89\n",
      "\tAccuracy: 0.739583333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 96\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 105\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 113\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 116\n",
      "\tAccuracy: 0.802083333333\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 127\n",
      "\tAccuracy: 0.708333333333\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 133\n",
      "\tAccuracy: 0.677083333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 144\n",
      "\tAccuracy: 0.697916666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 148\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 178\n",
      "\tAccuracy: 0.739583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 217\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 237\n",
      "\tAccuracy: 0.625\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 239\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 265\n",
      "\tAccuracy: 0.78125\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 276\n",
      "\tAccuracy: 0.697916666667\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 285\n",
      "\tAccuracy: 0.71875\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 289\n",
      "\tAccuracy: 0.770833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 291\n",
      "\tAccuracy: 0.802083333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 293\n",
      "\tAccuracy: 0.739583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 296\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 307\n",
      "\tAccuracy: 0.760416666667\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 325\n",
      "\tAccuracy: 0.677083333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 327\n",
      "\tAccuracy: 0.729166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 358\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_rh = np.zeros((distances_baseline_rh.shape[0],3))\n",
    "for roicount in range(distances_baseline_rh.shape[0]):\n",
    "    ntrials = len(subjNums)*2\n",
    "    p = stats.binom_test(np.mean(distances_baseline_rh[roicount,:])*ntrials,n=ntrials,p=0.5)\n",
    "    if np.mean(distances_baseline_rh[roicount,:])>0.5:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0-p/2.0\n",
    "\n",
    "    statistics_rh[roicount,0] = np.mean(distances_baseline_rh[roicount,:])\n",
    "    statistics_rh[roicount,1] = p\n",
    "\n",
    "# rois_testing = []\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "rois_testing = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "h0, qs = mc.fdrcorrection0(statistics_rh[rois_testing,1])\n",
    "h0 = (statistics_rh[rois_testing,1]*len(rois_testing))<0.05\n",
    "statistics_rh[:,1] = 1.0\n",
    "i = 0\n",
    "for roi in rois_testing:\n",
    "    statistics_rh[roi,1] = qs[i]\n",
    "    statistics_rh[roi,2] = h0[i]*statistics_rh[roi,0]\n",
    "    i += 1\n",
    "        \n",
    "nSignificant = np.sum(statistics_rh[:,1] < 0.05)\n",
    "print 'Number of significant transfers:', nSignificant\n",
    "\n",
    "if nSignificant>0:\n",
    "    sig_ind = np.where(statistics_rh[:,1]<0.05)[0]\n",
    "    print 'Number of significant effects', nSignificant\n",
    "    print 'Left hemisphere effects:', np.sum(sig_ind<180)\n",
    "    print 'Right hemisphere effects:', np.sum(sig_ind>180)\n",
    "    for ind in sig_ind:\n",
    "        print '\\tSignificant parcel:', ind+1\n",
    "        print '\\tAccuracy:', statistics_rh[ind,0]\n",
    "        print '\\tNetwork:', networkdef[ind]\n",
    "        \n",
    "        \n",
    "decodingROI = np.zeros((glasser2.shape[0],4)) # unthresh acc, thresh acc, t, q\n",
    "sig_acc = np.multiply(statistics_rh[:,0],statistics_rh[:,1]<0.05)\n",
    "for roi in range(nParcels):\n",
    "    roi_ind = np.where(glasser2==roi+1)[0]\n",
    "\n",
    "    decodingROI[roi_ind,0] = statistics_rh[roi,0] - 0.5\n",
    "    decodingROI[roi_ind,1] = sig_acc[roi] - 0.5\n",
    "    decodingROI[roi_ind,2] = statistics_rh[roi,1]\n",
    "    decodingROI[roi_ind,3] = statistics_rh[roi,2]\n",
    "\n",
    "# Compute effect size baseline (information content)\n",
    "outdir = '/projects3/SRActFlow/data/results/fMRI9a_GroupMotorDecoding/'\n",
    "filename = 'RegionToRegionMotorDecoding_RH_64k'\n",
    "np.savetxt(outdir + filename + '.csv', decodingROI,fmt='%s')\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + filename + '.dscalar.nii -reset-scalars'\n",
    "os.system(wb_command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.01 Load data for LH responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsr = True\n",
    "nResponses = 2\n",
    "data_task_lh = np.zeros((len(glasser2),nResponses,len(subjNums)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    data_task_lh[:,:,scount] = tools.loadCrossTrialMotorResponses(subj,hand='Left')\n",
    "    scount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.02 Generate actflow data for RH responses (Left S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_lh = 189 #left S1\n",
    "target_lh_ind = np.where(glasser2==roi_lh)[0]\n",
    "fcmapping_lh = np.zeros((len(source_ind),len(target_lh_ind)))\n",
    "\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    fcmapping_lh[:,:] = fcmapping_lh[:,:] + tools.loadPcaFCNoColliders(subj,roi_lh)\n",
    "    scount += 1\n",
    "\n",
    "fcmapping_lh = np.divide(fcmapping_lh,len(subjNums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 013 ( 1 / 96 )\n",
      "Subject 014 ( 2 / 96 )\n",
      "Subject 016 ( 3 / 96 )\n",
      "Subject 017 ( 4 / 96 )\n",
      "Subject 018 ( 5 / 96 )\n",
      "Subject 021 ( 6 / 96 )\n",
      "Subject 023 ( 7 / 96 )\n",
      "Subject 024 ( 8 / 96 )\n",
      "Subject 026 ( 9 / 96 )\n",
      "Subject 027 ( 10 / 96 )\n",
      "Subject 028 ( 11 / 96 )\n",
      "Subject 030 ( 12 / 96 )\n",
      "Subject 031 ( 13 / 96 )\n",
      "Subject 032 ( 14 / 96 )\n",
      "Subject 033 ( 15 / 96 )\n",
      "Subject 034 ( 16 / 96 )\n",
      "Subject 035 ( 17 / 96 )\n",
      "Subject 037 ( 18 / 96 )\n",
      "Subject 038 ( 19 / 96 )\n",
      "Subject 039 ( 20 / 96 )\n",
      "Subject 040 ( 21 / 96 )\n",
      "Subject 041 ( 22 / 96 )\n",
      "Subject 042 ( 23 / 96 )\n",
      "Subject 043 ( 24 / 96 )\n",
      "Subject 045 ( 25 / 96 )\n",
      "Subject 046 ( 26 / 96 )\n",
      "Subject 047 ( 27 / 96 )\n",
      "Subject 048 ( 28 / 96 )\n",
      "Subject 049 ( 29 / 96 )\n",
      "Subject 050 ( 30 / 96 )\n",
      "Subject 053 ( 31 / 96 )\n",
      "Subject 055 ( 32 / 96 )\n",
      "Subject 056 ( 33 / 96 )\n",
      "Subject 057 ( 34 / 96 )\n",
      "Subject 058 ( 35 / 96 )\n",
      "Subject 062 ( 36 / 96 )\n",
      "Subject 063 ( 37 / 96 )\n",
      "Subject 066 ( 38 / 96 )\n",
      "Subject 067 ( 39 / 96 )\n",
      "Subject 068 ( 40 / 96 )\n",
      "Subject 069 ( 41 / 96 )\n",
      "Subject 070 ( 42 / 96 )\n",
      "Subject 072 ( 43 / 96 )\n",
      "Subject 074 ( 44 / 96 )\n",
      "Subject 075 ( 45 / 96 )\n",
      "Subject 076 ( 46 / 96 )\n",
      "Subject 077 ( 47 / 96 )\n",
      "Subject 081 ( 48 / 96 )\n",
      "Subject 085 ( 49 / 96 )\n",
      "Subject 086 ( 50 / 96 )\n",
      "Subject 087 ( 51 / 96 )\n",
      "Subject 088 ( 52 / 96 )\n",
      "Subject 090 ( 53 / 96 )\n",
      "Subject 092 ( 54 / 96 )\n",
      "Subject 093 ( 55 / 96 )\n",
      "Subject 094 ( 56 / 96 )\n",
      "Subject 095 ( 57 / 96 )\n",
      "Subject 097 ( 58 / 96 )\n",
      "Subject 098 ( 59 / 96 )\n",
      "Subject 099 ( 60 / 96 )\n",
      "Subject 101 ( 61 / 96 )\n",
      "Subject 102 ( 62 / 96 )\n",
      "Subject 103 ( 63 / 96 )\n",
      "Subject 104 ( 64 / 96 )\n",
      "Subject 105 ( 65 / 96 )\n",
      "Subject 106 ( 66 / 96 )\n",
      "Subject 108 ( 67 / 96 )\n",
      "Subject 109 ( 68 / 96 )\n",
      "Subject 110 ( 69 / 96 )\n",
      "Subject 111 ( 70 / 96 )\n",
      "Subject 112 ( 71 / 96 )\n",
      "Subject 114 ( 72 / 96 )\n",
      "Subject 115 ( 73 / 96 )\n",
      "Subject 117 ( 74 / 96 )\n",
      "Subject 119 ( 75 / 96 )\n",
      "Subject 120 ( 76 / 96 )\n",
      "Subject 121 ( 77 / 96 )\n",
      "Subject 122 ( 78 / 96 )\n",
      "Subject 123 ( 79 / 96 )\n",
      "Subject 124 ( 80 / 96 )\n",
      "Subject 125 ( 81 / 96 )\n",
      "Subject 126 ( 82 / 96 )\n",
      "Subject 127 ( 83 / 96 )\n",
      "Subject 128 ( 84 / 96 )\n",
      "Subject 129 ( 85 / 96 )\n",
      "Subject 130 ( 86 / 96 )\n",
      "Subject 131 ( 87 / 96 )\n",
      "Subject 132 ( 88 / 96 )\n",
      "Subject 134 ( 89 / 96 )\n",
      "Subject 135 ( 90 / 96 )\n",
      "Subject 136 ( 91 / 96 )\n",
      "Subject 137 ( 92 / 96 )\n",
      "Subject 138 ( 93 / 96 )\n",
      "Subject 139 ( 94 / 96 )\n",
      "Subject 140 ( 95 / 96 )\n",
      "Subject 141 ( 96 / 96 )\n"
     ]
    }
   ],
   "source": [
    "roi_rh = 9 # left s1\n",
    "roi_lh = 189 # right s1\n",
    "actflow_data = np.zeros((len(glasser2),nResponses,len(subjNums),nParcels))\n",
    "\n",
    "dilateLH = tools.loadMask(roi_lh,dilated=True)\n",
    "dilateRH = tools.loadMask(roi_rh,dilated=True)\n",
    "combinedDilated = dilateLH + dilateRH\n",
    "# Exclude all SMN regions\n",
    "smn_rois = np.where(networkdef==networkmappings['smn'])[0]\n",
    "for x in smn_rois:\n",
    "    roi_ind = np.where(glasser2==x)[0]\n",
    "    combinedDilated[roi_ind]=1\n",
    "source_space = np.where(combinedDilated==0)[0]\n",
    "\n",
    "fc_source_ind = glasser2[source_space].copy()\n",
    "\n",
    "bad_rois = []\n",
    "scount = 0\n",
    "for subj in subjNums:\n",
    "    print 'Subject', subj, '(', scount+1, '/', len(subjNums), ')'\n",
    "    for roi in range(nParcels):\n",
    "        source_ind = np.where(glasser2==roi+1)[0]\n",
    "        source_ind = np.intersect1d(source_ind,source_space) # Make sure no vertices are within 10mm of the target\n",
    "        if source_ind.shape[0]==0:\n",
    "#             print 'This source has no vertices, skipping'\n",
    "            bad_rois.append(roi)\n",
    "            continue\n",
    "        \n",
    "        fc_source_roi = np.where(fc_source_ind==roi+1)[0]\n",
    "        \n",
    "        target_ind = np.where(glasser2==roi_lh)[0]\n",
    "#         # Left Finger 1\n",
    "#         actflow_data[target_ind,0,scount,roi] = np.dot(stats.zscore(data_task_lh[source_ind,0,scount],axis=0),fcmapping_lh[fc_source_roi,:])\n",
    "#         # Left Finger 2\n",
    "#         actflow_data[target_ind,1,scount,roi] = np.dot(stats.zscore(data_task_lh[source_ind,1,scount],axis=0),fcmapping_lh[fc_source_roi,:])\n",
    "        # Left Finger 1\n",
    "        actflow_data[target_ind,0,scount,roi] = np.dot(data_task_lh[source_ind,0,scount],fcmapping_lh[fc_source_roi,:])\n",
    "        # Left Finger 2\n",
    "        actflow_data[target_ind,1,scount,roi] = np.dot(data_task_lh[source_ind,1,scount],fcmapping_lh[fc_source_roi,:])\n",
    "\n",
    "    scount += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run across subject decoding on left-hand motor responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding roi 13\n",
      "Decoding roi 14\n",
      "Decoding roi 28\n",
      "Decoding roi 62\n",
      "Decoding roi 72\n",
      "Decoding roi 76\n",
      "Decoding roi 79\n",
      "Decoding roi 81\n",
      "Decoding roi 82\n",
      "Decoding roi 84\n",
      "Decoding roi 88\n",
      "Decoding roi 90\n",
      "Decoding roi 91\n",
      "Decoding roi 96\n",
      "Decoding roi 97\n",
      "Decoding roi 110\n",
      "Decoding roi 132\n",
      "Decoding roi 143\n",
      "Decoding roi 144\n",
      "Decoding roi 148\n",
      "Decoding roi 169\n",
      "Decoding roi 170\n",
      "Decoding roi 193\n",
      "Decoding roi 194\n",
      "Decoding roi 208\n",
      "Decoding roi 237\n",
      "Decoding roi 241\n",
      "Decoding roi 242\n",
      "Decoding roi 252\n",
      "Decoding roi 253\n",
      "Decoding roi 256\n",
      "Decoding roi 259\n",
      "Decoding roi 260\n",
      "Decoding roi 262\n",
      "Decoding roi 264\n",
      "Decoding roi 268\n",
      "Decoding roi 270\n",
      "Decoding roi 271\n",
      "Decoding roi 272\n",
      "Decoding roi 276\n",
      "Decoding roi 277\n",
      "Decoding roi 290\n",
      "Decoding roi 312\n",
      "Decoding roi 323\n",
      "Decoding roi 324\n",
      "Decoding roi 328\n",
      "Decoding roi 341\n",
      "Decoding roi 349\n",
      "Decoding roi 350\n",
      "Decoding roi 356\n",
      "Decoding roi 10\n",
      "Decoding roi 45\n",
      "Decoding roi 49\n",
      "Decoding roi 94\n",
      "Decoding roi 95\n",
      "Decoding roi 115\n",
      "Decoding roi 116\n",
      "Decoding roi 126\n",
      "Decoding roi 135\n",
      "Decoding roi 136\n",
      "Decoding roi 142\n",
      "Decoding roi 145\n",
      "Decoding roi 225\n",
      "Decoding roi 229\n",
      "Decoding roi 274\n",
      "Decoding roi 275\n",
      "Decoding roi 295\n",
      "Decoding roi 296\n",
      "Decoding roi 306\n",
      "Decoding roi 315\n",
      "Decoding roi 316\n",
      "Decoding roi 322\n",
      "Decoding roi 325\n",
      "Decoding roi 9\n",
      "Decoding roi 36\n",
      "Decoding roi 37\n",
      "Decoding roi 42\n",
      "Decoding roi 43\n",
      "Decoding roi 44\n",
      "Decoding roi 56\n",
      "Decoding roi 57\n",
      "Decoding roi 58\n",
      "Decoding roi 59\n",
      "Decoding roi 77\n",
      "Decoding roi 83\n",
      "Decoding roi 85\n",
      "Decoding roi 98\n",
      "Decoding roi 104\n",
      "Decoding roi 105\n",
      "Decoding roi 107\n",
      "Decoding roi 108\n",
      "Decoding roi 112\n",
      "Decoding roi 113\n",
      "Decoding roi 146\n",
      "Decoding roi 147\n",
      "Decoding roi 166\n",
      "Decoding roi 168\n",
      "Decoding roi 177\n",
      "Decoding roi 178\n",
      "Decoding roi 179\n",
      "Decoding roi 189\n",
      "Decoding roi 190\n",
      "Decoding roi 204\n",
      "Decoding roi 216\n",
      "Decoding roi 217\n",
      "Decoding roi 222\n",
      "Decoding roi 223\n",
      "Decoding roi 224\n",
      "Decoding roi 236\n",
      "Decoding roi 238\n",
      "Decoding roi 239\n",
      "Decoding roi 257\n",
      "Decoding roi 261\n",
      "Decoding roi 263\n",
      "Decoding roi 265\n",
      "Decoding roi 278\n",
      "Decoding roi 284\n",
      "Decoding roi 285\n",
      "Decoding roi 287\n",
      "Decoding roi 288\n",
      "Decoding roi 292\n",
      "Decoding roi 293\n",
      "Decoding roi 326\n",
      "Decoding roi 327\n",
      "Decoding roi 346\n",
      "Decoding roi 348\n",
      "Decoding roi 357\n",
      "Decoding roi 358\n",
      "Decoding roi 359\n"
     ]
    }
   ],
   "source": [
    "nproc = 20\n",
    "ncvs = 1\n",
    "\n",
    "rois = np.where(networkdef==networkmappings['smn'])[0] + 1\n",
    "rois = [189] # Right S1\n",
    "\n",
    "\n",
    "sourceROIs = []\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "sourceROIs.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "# sourceROIs = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "\n",
    "distances_baseline_lh = np.zeros((nParcels,len(subjNums)*2))\n",
    "for roi in sourceROIs:\n",
    "    print 'Decoding roi', roi\n",
    "    if roi in bad_rois: \n",
    "        continue\n",
    "\n",
    "    distances_baseline_lh[roi,:] = motorResponseDecodings(data_task_lh,\n",
    "                                                          actflow_data[:,:,:,roi],\n",
    "                                                          rois=rois, ncvs=ncvs, nproc=nproc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant transfers: 29\n",
      "Number of significant effects 29\n",
      "Left hemisphere effects: 11\n",
      "Right hemisphere effects: 18\n",
      "\tSignificant parcel: 77\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 78\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 80\n",
      "\tAccuracy: 0.770833333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 85\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 133\n",
      "\tAccuracy: 0.625\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 137\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 143\n",
      "\tAccuracy: 0.708333333333\n",
      "\tNetwork: 5.0\n",
      "\tSignificant parcel: 144\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 147\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 169\n",
      "\tAccuracy: 0.729166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 180\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 190\n",
      "\tAccuracy: 0.8125\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 205\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 223\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 254\n",
      "\tAccuracy: 0.645833333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 258\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 260\n",
      "\tAccuracy: 0.614583333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 265\n",
      "\tAccuracy: 0.708333333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 269\n",
      "\tAccuracy: 0.635416666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 273\n",
      "\tAccuracy: 0.833333333333\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 278\n",
      "\tAccuracy: 0.697916666667\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 288\n",
      "\tAccuracy: 0.739583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 289\n",
      "\tAccuracy: 0.604166666667\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 291\n",
      "\tAccuracy: 0.8125\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 293\n",
      "\tAccuracy: 0.739583333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 313\n",
      "\tAccuracy: 0.65625\n",
      "\tNetwork: 7.0\n",
      "\tSignificant parcel: 328\n",
      "\tAccuracy: 0.65625\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 349\n",
      "\tAccuracy: 0.677083333333\n",
      "\tNetwork: 4.0\n",
      "\tSignificant parcel: 359\n",
      "\tAccuracy: 0.666666666667\n",
      "\tNetwork: 4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_lh = np.zeros((distances_baseline_lh.shape[0],3))\n",
    "for roicount in range(distances_baseline_lh.shape[0]):\n",
    "    ntrials = len(subjNums)*2\n",
    "    p = stats.binom_test(np.mean(distances_baseline_lh[roicount,:])*ntrials,n=ntrials,p=0.5)\n",
    "    if np.mean(distances_baseline_lh[roicount,:])>0.5:\n",
    "        p = p/2.0\n",
    "    else:\n",
    "        p = 1.0-p/2.0\n",
    "\n",
    "    statistics_lh[roicount,0] = np.mean(distances_baseline_lh[roicount,:])\n",
    "    statistics_lh[roicount,1] = p\n",
    "\n",
    "# rois_testing = []\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['fpn'])[0])\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['con'])[0])\n",
    "# rois_testing.extend(np.where(networkdef==networkmappings['dan'])[0])\n",
    "rois_testing = np.where(networkdef!=networkmappings['smn'])[0]\n",
    "h0, qs = mc.fdrcorrection0(statistics_lh[rois_testing,1])\n",
    "h0 = (statistics_lh[rois_testing,1]*len(rois_testing))<0.05\n",
    "statistics_lh[:,1] = 1.0\n",
    "i = 0\n",
    "for roi in rois_testing:\n",
    "    statistics_lh[roi,1] = qs[i]\n",
    "    statistics_lh[roi,2] = h0[i]*statistics_lh[roi,0]\n",
    "    i += 1\n",
    "        \n",
    "nSignificant = np.sum(statistics_lh[:,1] < 0.05)\n",
    "print 'Number of significant transfers:', nSignificant\n",
    "\n",
    "if nSignificant>0:\n",
    "    sig_ind = np.where(statistics_lh[:,1]<0.05)[0]\n",
    "    print 'Number of significant effects', nSignificant\n",
    "    print 'Left hemisphere effects:', np.sum(sig_ind<180)\n",
    "    print 'Right hemisphere effects:', np.sum(sig_ind>180)\n",
    "    for ind in sig_ind:\n",
    "        print '\\tSignificant parcel:', ind+1\n",
    "        print '\\tAccuracy:', statistics_lh[ind,0]\n",
    "        print '\\tNetwork:', networkdef[ind]\n",
    "\n",
    "decodingROI = np.zeros((glasser2.shape[0],4)) # unthresh acc, thresh acc, t, q\n",
    "sig_acc = np.multiply(statistics_lh[:,0],statistics_lh[:,1]<0.05)\n",
    "for roi in range(nParcels):\n",
    "    roi_ind = np.where(glasser2==roi+1)[0]\n",
    "\n",
    "    decodingROI[roi_ind,0] = statistics_lh[roi,0] - 0.5\n",
    "    decodingROI[roi_ind,1] = sig_acc[roi] - 0.5\n",
    "    decodingROI[roi_ind,2] = statistics_lh[roi,1]\n",
    "    decodingROI[roi_ind,3] = statistics_lh[roi,2]\n",
    "\n",
    "# Compute effect size baseline (information content)\n",
    "outdir = '/projects3/SRActFlow/data/results/fMRI9a_GroupMotorDecoding/'\n",
    "filename = 'RegionToRegionMotorDecoding_LH_64k'\n",
    "np.savetxt(outdir + filename + '.csv', decodingROI,fmt='%s')\n",
    "wb_command = 'wb_command -cifti-convert -from-text ' + outdir + filename + '.csv ' + glasserfile2 + ' ' + outdir + filename + '.dscalar.nii -reset-scalars'\n",
    "os.system(wb_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
