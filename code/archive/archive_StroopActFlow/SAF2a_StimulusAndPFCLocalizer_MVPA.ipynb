{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StroopActFlow\n",
    "### Identify stimuli localizers for input layer for Stroop Model\n",
    "### Identify top-down task rule inputs (e.g., PFC) for Stroop Model\n",
    "\n",
    "\n",
    "#### Taku Ito\n",
    "#### 1/17/2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import statsmodels.sandbox.stats.multicomp as mc\n",
    "import os\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertCSVToCIFTI64k(inputfilename,outputfilename):\n",
    "    ciftitemplate = '/projects3/StroopActFlow/data/Q1-Q6_RelatedParcellation210.LR.CorticalAreas_dil_Colors.32k_fs_LR.dlabel.nii'\n",
    "    wb_command = 'wb_command -cifti-convert -from-text' \n",
    "    wb_command += ' ' + inputfilename \n",
    "    wb_command += ' ' + ciftitemplate\n",
    "    wb_command += ' ' + outputfilename\n",
    "    wb_command += \" -col-delim ','\"\n",
    "    wb_command += ' -reset-scalars'\n",
    "    os.system(wb_command)\n",
    "#     print wb_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjNums = ['101', '102']\n",
    "\n",
    "basedir = '/projects3/StroopActFlow/data/'\n",
    "resultsdir = basedir + 'results/'\n",
    "restdir = resultsdir + 'glmRest_GlasserParcels/'\n",
    "\n",
    "glasser_nets = np.loadtxt('/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/community_order.txt', delimiter=',')\n",
    "# Make into python numbering (starts from 0)\n",
    "glasser_nets -= 1.0\n",
    "networkorder = glasser_nets.astype(int)\n",
    "networkorder.shape = (len(networkorder),1)\n",
    "\n",
    "networkmappings = {'fpn':7, 'vis':1, 'smn':2, 'con':3, 'dmn':6, 'aud1':8, 'aud2':9, 'dan':11}\n",
    "networks = networkmappings.keys()\n",
    "\n",
    "networkdef = '/projects/AnalysisTools/netpartitions/ColeLabNetPartition_v1.1/parcel_network_assignments.txt'\n",
    "networkdef = np.loadtxt(networkdef, delimiter=',')\n",
    "xticks = {}\n",
    "reorderednetworkaffil = networkdef[networkorder]\n",
    "for net in networks:\n",
    "    netNum = networkmappings[net]\n",
    "    netind = np.where(reorderednetworkaffil==netNum)[0]\n",
    "    tick = np.max(netind)\n",
    "    xticks[tick] = net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Run task activation analysis on 64k Surface for four different stimuli localizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBetas(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "    # Load smoothed betas!\n",
    "    betas = np.loadtxt(datadir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k_sm.csv',delimiter=',')\n",
    "    betas = betas[:,18:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBetasFromDscalar(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "    # Load smoothed betas!\n",
    "    betas = nib.load(datadir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k_sm.dscalar.nii')\n",
    "    betas = betas.get_data()\n",
    "    betas = np.squeeze(betas)\n",
    "    betas = betas.T\n",
    "    betas = betas[:,18:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Convert CSV to Dscalar and then run smoothing on beta maps\n",
    "\n",
    "* Smoothing with 4mm FWHM on surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 101\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n",
      "Subject 102\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n"
     ]
    }
   ],
   "source": [
    "for subj in subjNums:\n",
    "    print 'Subject', subj\n",
    "    basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "    \n",
    "    print 'Converting CSV to dscalar...'\n",
    "    csvfile = basedir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k.csv'\n",
    "    dscalarfile = basedir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k.dscalar.nii'\n",
    "    convertCSVToCIFTI64k(csvfile, dscalarfile)\n",
    "    \n",
    "    print 'Smoothing surface files...'\n",
    "    dscalar_sm_file = basedir + subj + '_neutralStimuliByTrial_taskBetas_Surface64k_sm.dscalar.nii'\n",
    "    subjdir = '/projects3/StroopActFlow/data/' + subj + '/MNINonLinear/fsaverage_LR32k/'\n",
    "    lsurf = subjdir + subj + '.L.inflated.32k_fs_LR.surf.gii'\n",
    "    rsurf = subjdir + subj + '.R.inflated.32k_fs_LR.surf.gii'\n",
    "    \n",
    "    wb_command = 'wb_command -cifti-smoothing ' + dscalarfile + ' 4 4 COLUMN ' + dscalar_sm_file\n",
    "    wb_command += ' -left-surface ' + lsurf\n",
    "    wb_command += ' -right-surface ' + rsurf\n",
    "    os.system(wb_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Run statistical analysis (i.e., activation threshold analysis) against 0\n",
    "* Stim 1: Neutral Color Green\n",
    "* Stim 2: Neutral Color Red\n",
    "* Stim 3: Neutral Word Green\n",
    "* Stim 4: Neutral Word Red\n",
    "\n",
    "Betas are z-scored prior to running t-tests against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 45\n",
    "nstims = 4\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalar(subj)\n",
    "    betas = stats.zscore(betas,axis=0)\n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims + 1)) # Include conjunction\n",
    "    \n",
    "    i=0\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "        for trial in range(ntrials_perstim):\n",
    "            beta_mat[:,trial] = betas[:,i]\n",
    "            i += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_1samp(beta_mat[vertex,:],0)\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim]) \n",
    "        # Include conjunction map\n",
    "        tmat_fdr[subj][:,4] = tmat_fdr[subj][:,4] + h0\n",
    "        \n",
    "    tmat_fdr[subj][:,4] = tmat_fdr[subj][:,4] == nstims\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_neutralStimuliByTrial_tstats.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run statistical analysis (i.e., activation threshold analysis) with contrasts against other stimulus conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Thresholding statistical maps for stim 2\n",
      "Thresholding statistical maps for stim 3\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 45\n",
    "nstims = 4\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalar(subj)\n",
    "    betas = stats.zscore(betas,axis=0)    \n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(4,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(a,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "        beta_mat_contrast = np.zeros((betas.shape[0],ntrials_perstim*(nstims-1)))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "        \n",
    "        notcond_ind = np.where(stim_ind!=stim)[0]\n",
    "        beta_mat_contrast[:,:] = betas[:,notcond_ind]        \n",
    "        \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_ind(beta_mat[vertex,:],beta_mat_contrast[vertex,:])\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim]) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_neutral_stimuli/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_neutralStimuliByTrial_tstats_StimVAll.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_neutralStimuliByTrial_tstatsThresh_StimVAll.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Run task-rule encoding against 0\n",
    "### Isolate top-down control (i.e., hypothesized PFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadBetasFromDscalarTaskRuleEncoding(subj):\n",
    "    datadir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "    # Load smoothed betas!\n",
    "    betas = nib.load(datadir + subj + '_taskRuleEncoding_taskBetas_Surface64k_sm.dscalar.nii')\n",
    "    betas = betas.get_data()\n",
    "    betas = np.squeeze(betas)\n",
    "    betas = betas.T\n",
    "    betas = betas[:,18:] # 18 onwards are the two task betas\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 101\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n",
      "Subject 102\n",
      "Converting CSV to dscalar...\n",
      "Smoothing surface files...\n"
     ]
    }
   ],
   "source": [
    "for subj in subjNums:\n",
    "    print 'Subject', subj\n",
    "    glmdir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/' \n",
    "    \n",
    "    print 'Converting CSV to dscalar...'\n",
    "    csvfile = glmdir + subj + '_taskruleEncoding_taskBetas_Surface64k.csv'\n",
    "    dscalarfile = glmdir + subj + '_taskRuleEncoding_taskBetas_Surface64k.dscalar.nii'\n",
    "    convertCSVToCIFTI64k(csvfile, dscalarfile)\n",
    "    \n",
    "    print 'Smoothing surface files...'\n",
    "    dscalar_sm_file = glmdir + subj + '_taskRuleEncoding_taskBetas_Surface64k_sm.dscalar.nii'\n",
    "    subjdir = '/projects3/StroopActFlow/data/' + subj + '/MNINonLinear/fsaverage_LR32k/'\n",
    "    lsurf = subjdir + subj + '.L.inflated.32k_fs_LR.surf.gii'\n",
    "    rsurf = subjdir + subj + '.R.inflated.32k_fs_LR.surf.gii'\n",
    "    \n",
    "    wb_command = 'wb_command -cifti-smoothing ' + dscalarfile + ' 4 4 COLUMN ' + dscalar_sm_file\n",
    "    wb_command += ' -left-surface ' + lsurf\n",
    "    wb_command += ' -right-surface ' + rsurf\n",
    "    os.system(wb_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.1 Run statistical analysis (i.e., activation threshold analysis) against 0\n",
    "* Task 1: ColorEncoding\n",
    "* Task 2: WordEncoding\n",
    "\n",
    "Betas are z-scored prior to running t-tests against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 30\n",
    "nstims = 2\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalarTaskRuleEncoding(subj)\n",
    "    betas = stats.zscore(betas,axis=0)\n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims + 1)) # Include conjunction\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(nstims,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(stim_ind,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "            \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_1samp(beta_mat[vertex,:],0)\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim])         \n",
    "        # Include conjunction map\n",
    "        tmat_fdr[subj][:,2] = tmat_fdr[subj][:,2] + h0\n",
    "        \n",
    "    tmat_fdr[subj][:,2] = tmat_fdr[subj][:,2] == nstims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_taskRuleEncoding_tstats.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_taskRuleEncoding_tstatsThresh.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Run task-rule encoding contrasts (color v. word)\n",
    "### Isolate top-down control (i.e., hypothesized PFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Run statistical analysis (i.e., activation threshold analysis) against 0\n",
    "* Task 1: ColorEncoding\n",
    "* Task 2: WordEncoding\n",
    "\n",
    "Betas are z-scored prior to running t-tests against 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running within-subject analysis on subj 101\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n",
      "Running within-subject analysis on subj 102\n",
      "Thresholding statistical maps for stim 0\n",
      "Thresholding statistical maps for stim 1\n"
     ]
    }
   ],
   "source": [
    "ntrials_perstim = 30\n",
    "nstims = 2\n",
    "\n",
    "tmat = {}\n",
    "pmat = {}\n",
    "qmat = {}\n",
    "tmat_fdr = {}\n",
    "\n",
    "for subj in subjNums:\n",
    "    print 'Running within-subject analysis on subj', subj\n",
    "    \n",
    "    betas = loadBetasFromDscalarTaskRuleEncoding(subj)\n",
    "    betas = stats.zscore(betas,axis=0)\n",
    "    tmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    pmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    qmat[subj] = np.zeros((betas.shape[0],nstims))\n",
    "    tmat_fdr[subj] = np.zeros((betas.shape[0],nstims)) # Include conjunction\n",
    "    \n",
    "    # Construct index for which stims corresopnd to which columns\n",
    "    stim_ind = np.arange(nstims,dtype=np.ndarray)\n",
    "    stim_ind.shape = (stim_ind.shape[0],1)\n",
    "    stim_ind = np.reshape(np.tile(stim_ind,ntrials_perstim),-1)\n",
    "    for stim in range(nstims):\n",
    "        print 'Thresholding statistical maps for stim', stim\n",
    "        beta_mat = np.zeros((betas.shape[0],ntrials_perstim))\n",
    "        beta_mat_contrast = np.zeros((betas.shape[0],ntrials_perstim*(nstims-1)))\n",
    "                          \n",
    "        cond_ind = np.where(stim_ind==stim)[0]\n",
    "        beta_mat[:,:] = betas[:,cond_ind]\n",
    "      \n",
    "        notcond_ind = np.where(stim_ind!=stim)[0]\n",
    "        beta_mat_contrast[:,:] = betas[:,notcond_ind]        \n",
    "      \n",
    "        # Run t-test\n",
    "        t = []\n",
    "        p = []\n",
    "        for vertex in range(beta_mat.shape[0]):\n",
    "            t, p = stats.ttest_rel(beta_mat[vertex,:],beta_mat_contrast[vertex,:])\n",
    "            \n",
    "            # Make one-sided t-test\n",
    "            if t > 0:\n",
    "                p = p/2.0\n",
    "            else:\n",
    "                p = 1.0 - p/2.0\n",
    "                \n",
    "            tmat[subj][vertex,stim] = t\n",
    "            pmat[subj][vertex,stim] = p\n",
    "            \n",
    "\n",
    "            \n",
    "        # Run FDR correction\n",
    "        h0, qmat[subj][:,stim] = mc.fdrcorrection0(pmat[subj][:,stim])\n",
    "        # Threshold tmat for significance\n",
    "        tmat_fdr[subj][:,stim] = np.multiply(h0,tmat[subj][:,stim])         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.csv'\n",
    "        \n",
    "    np.savetxt(filename_tmat, tmat[subj], delimiter=',')\n",
    "    np.savetxt(filename_tmatfdr, tmat_fdr[subj], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert statistical maps to surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/projects3/StroopActFlow/data/results/glm_taskruleencoding/'\n",
    "for subj in subjNums:\n",
    "    \n",
    "    filename_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.csv'\n",
    "    filename_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.csv'\n",
    "    \n",
    "    cifti_tmat = basedir + subj + '_taskRuleEncoding_ruleContrast_tstats.dscalar.nii'\n",
    "    cifti_tmatfdr = basedir + subj + '_taskRuleEncoding_ruleContrast_tstatsThresh.dscalar.nii'\n",
    "    \n",
    "    convertCSVToCIFTI64k(filename_tmat, cifti_tmat)\n",
    "    convertCSVToCIFTI64k(filename_tmatfdr, cifti_tmatfdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "* No significant voxels for word v. color rule contrasts (within subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
